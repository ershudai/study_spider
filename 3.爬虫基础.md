# 爬虫

## 爬虫简介及基础使用

模拟人为操作触发的网络请求，获取特定数据。

### 爬虫的工作流程

#### 1.确定目标地址

确定信息来源的地址

#### 2.发送请求

根据特定请求方式，使用代码发送请求，获得响应

#### 3.提取信息

提取响应中的数据-python提取数据的方案：Beautiful Soup、pyquery、lxml、json、正则等等，根据响应的数据选择工具

#### 4.数据存储

存储数据---本地持久化生成文件txt、json等。数据库存储mysql、mongoDB等。远程传输ftp、sftp等

### python网络交互模块

1. **`socket` 模块**：
   - 这是 Python 的一个低级网络接口，提供了对套接字编程的支持。它是所有更高层次网络通信的基础，包括 HTTP 请求。
2. **`http.client` 模块**：
   - `http.client` 是 Python 标准库中的一个模块，它提供了一个类 `HTTPConnection` 和 `HTTPSConnection`，用于与 HTTP/HTTPS 服务器进行交互。这个模块负责创建 TCP 连接、发送 HTTP 请求、接收和解析 HTTP 响应等任务。它内部使用了 `socket` 模块来处理实际的网络通信。
3. **`urllib` 包**：
   - `urllib` 是另一个 Python 标准库，它提供了更高级别的 HTTP 请求处理功能。`urllib.request` 模块可以用来发出 HTTP 请求，并且支持 URL 编码、代理、重定向等功能。`urllib` 内部使用了 `http.client` 来处理 HTTP 协议的具体细节。
4. **`requests` 库**：
   - `requests` 是一个第三方库，旨在简化 HTTP 请求的发出过程。它构建在 `urllib` 和 `http.client` 之上，提供了更加用户友好的 API。`requests` 库抽象掉了许多底层的复杂性，使得开发者可以更容易地发出 GET、POST 等请求，处理响应，管理会话，以及执行其他常见的 HTTP 操作。它还添加了许多有用的功能，如自动内容解码、保持连接、身份验证、文件上传等。

常用还是requests，其他模块太原始使用复杂。

```python
import requests 

# 目标url
url = 'http://www.baidu.com' 

# 向目标url发送get请求
response = requests.get(url)

# 打印响应内容
print(response.text)
```



```python
import requests 

url = 'http://www.baidu.com' 
response = requests.get(url)
print(response.text)

r_text=response.text
r_content=response.content
r_status_code=response.status_code
r_request_headers=response.request.headers
r_headers=response.headers
r_cookies=response.cookies

print(f"Response Text: {r_text}")
# - `response.text`
#   - 类型：str
#   - 解码类型： requests模块自动根据HTTP 头部对响应的编码作出有根据的推测，推测的文本编码
#   - 如何修改编码方式：`response.encoding=”gbk”`
print(f"Response Content: {r_content}")
# - `response.content`
#   - 类型：bytes
#   - 解码类型： 没有指定
#   - 如何修改编码方式：`response.content.deocde(“utf8”)`

print(f"Response Status Code: {r_status_code}")
print(f"Request Headers: {r_request_headers}")
print(f"Response Headers: {r_headers}")
print(f"Cookies: {r_cookies}")

# 获取网页源码的通用方式：
# 1. `response.content.decode()`
# 2. `response.content.decode("GBK")`
# 3. `response.text`
```



```python
str_code = 'abc'
print(type(str_code))
# str转bytes
byte_code = str_code.encode()
print(type(byte_code))
```

```python
byte_code = b'abc'
print(type(byte_code))
# bytes转str
str_code = byte_code.decode()
print(type(str_code))
```



### 使用request库使用案例

```python
import requests

url = 'https://www.baidu.com'

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}

# 在请求头中带上User-Agent，模拟浏览器发送请求
response = requests.get(url, headers=headers) 

# print(response.content)

# 打印请求头信息
print(response.request.headers)
```

```python
import requests

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}

url = 'https://www.sogou.com/web?query=python'

# kw = {'query': 'python'}
# url中包含了请求参数，所以此时无需params
response = requests.get(url, headers=headers,params=kw)

response = requests.get(url, headers=headers)
```

较完整的POST请求：
```python
import requests

# 获取到url
url = 'http://www.cninfo.com.cn/new/disclosure'

cookies={
    'JSESSIONID':'204BABEA1EEB56A40A98F189227102C',
    'insert_cookie':'45380249',
    'routeId':'.uc1',
    '_sp_ses.2141':'*',
    '_sp_id.2141':'4d159bf3-398b-4dd5-8e07-78ee07cbcbf9.1662302323.3.1662383561.1662359150.afa633dd-c1a8-4d81-90a7-01b0c3630055'
}

#cookies可以单独写，也可以像下面一样写在请求头中。
headers = {
   'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36',
    "Cookie":"JSESSIONID=204BABEA1EEB56A40A98F189227102C2; insert_cookie=45380249; routeId=.uc1; _sp_ses.2141=*; _sp_id.2141=4d159bf3-398b-4dd5-8e07-78ee07cbcbf9.1662302323.3.1662383561.1662359150.afa633dd-c1a8-4d81-90a7-01b0c3630055"
}
data = {
'column':'szse_latest',
'pageNum':'2',
'pageSize':'30',
'sortName':'',
'sortType':'',
'clusterFlag':'true',
}

response = requests.post(url, headers=headers, data=data,cookies=cookies)
print(response.json())
```

### requests.session处理cookie

前面使用手动的方式使用cookie，requests 提供了一个叫做session类，来实现客户端和服务端的`会话保持`

会话保持有两个内涵：

- 保存cookie，下一次请求会带上前一次的cookie
- 实现和服务端的长连接，加快请求速度
- 

```python
import requests

# 创建一个会话对象
session = requests.session()
# 访问登录页面，获取CSRF token
login_url = 'https://github.com/login'
response = session.get(login_url)

# 假设我们使用BeautifulSoup解析HTML，找到CSRF token
from bs4 import BeautifulSoup
soup = BeautifulSoup(response.text, 'html.parser')
csrf_token = soup.find('input', {'name': 'authenticity_token'})['value']
print(f"CSRF Token: {csrf_token}")
# 构造登录数据
login_data = {
    'commit': 'Sign in',
    'utf8': '✓',
    'authenticity_token': csrf_token,
    'login': 'your_username',  # 替换为你的GitHub用户名
    'password': 'your_password'  # 替换为你的GitHub密码
}

# 提交登录表单
login_post_url = 'https://github.com/session'
response = session.post(login_post_url, data=login_data)

# 访问个人主页，验证登录状态
profile_url = 'https://github.com/your_username'  # 替换为你的GitHub用户名
response = session.get(profile_url)

if response.status_code == 200:
    print("登录成功！")
else:
    print("登录失败，请检查用户名和密码。")
    
# 访问某个私有仓库
private_repo_url = 'https://github.com/your_username/private-repo'  # 替换为实际的私有仓库URL
response = session.get(private_repo_url)

if response.status_code == 200:
    print("成功访问私有仓库！")
else:
    print("无法访问私有仓库，请检查权限设置。")
# 关闭会话
session.close()



# 或者使用上下文管理器
with requests.session() as session:
    # 执行上述步骤...
    pass
```



### 使用代理

从请求使用的协议可以分为：

- http代理
- https代理
- socket代理等

```python
   import requests

  proxies = {
        "http": "http://106.15.190.190:3128",
        }
  response = requests.get("http://httpbin.org/ip", proxies=proxies, timeout=3)
  print(response.text)
```



### 常见问题及处理

#### 证书不安全

报错信息：

```cmd
ssl.CertificateError ...
```

解决方案：verify=False

```python
import requests

url = "https://yjcclm.com/"
response = requests.get(url,verify=False)
```

#### 超时时间

一个请求的时间过长，请求超过timeout时间未响应就报错：

```python
response = requests.get(url,timeout=3)
```

这个方法还能够拿来检测代理ip的质量，如果一个代理ip在很长时间没有响应，那么添加超时之后也会报错，对应的这个ip就可以从代理ip池中删除

#### 防止数据遗漏问题

请求失败了报错、响应超时报错，导致数据获取失败问题。使用retrying模块的@retry装饰器实现重试。

```cmd
pip install retrying
```
- stop_max_attempt_number：指定最大重试次数。一旦超过此限制，即使函数仍然失败也不会再重试。
- wait_fixed：每次重试之间的固定等待时间（以毫秒为单位）。这有助于避免过于频繁的重试尝试。
- wait_random_min 和 wait_random_max：定义每次重试之间随机等待的时间范围。这对于减少并发冲突尤其有用。
- retry_on_exception：允许指定一个函数，用于判断哪些类型的异常应该触发重试。只有当异常符合指定条件时才会重试。
- retry_on_result：类似于 retry_on_exception，但它基于函数返回的结果而不是异常来决定是否重试。例如，你可以设置当返回值为 None 或空字符串时进行重试4。

```python
from retrying import retry
import requests

@retry
def fetch_data(stop_max_attempt_number=3):
    response = requests.get("https://example.com/api/data")
    if response.status_code != 200:
        print("Request failed, status code:", response.status_code)
        raise Exception("Failed to fetch data")
    else:
        print("Data fetched successfully")
        return response.json()

try:
    data = fetch_data()
    print("Received data:", data)
except Exception as e:
    print("Error occurred:", e)
```



```python
from retrying import retry

def should_retry_if_io_error(exception):
    """Return True if we should retry (in this case when it's an IOError), False otherwise"""
    return isinstance(exception, IOError)

@retry(retry_on_exception=should_retry_if_io_error, stop_max_attempt_number=3, wait_fixed=2000)
def might_io_error():
    print("Trying...")
    raise IOError("IO Error!")

try:
    might_io_error()
except Exception as e:
    print("Failed, even with retrying.")
```



## 数据提取、存储

python提取数据

| 抓取工具      | 运行速度 | 使用难度 | 安装配置     |
| ------------- | -------- | -------- | ------------ |
| 正则re        | 最快     | 较难     | 自带（内置） |
| BeautifulSoup | 慢       | 最简单   | 简单         |
| lxml          | 快       | 简单     | 简单         |



### 数据提取方法

#### 爬虫中数据的分类

在爬虫爬取的数据中有很多不同类型的数据,我们需要了解数据的不同类型来又规律的提取和解析数据.

- 结构化数据：json，xml等
  - 处理方式：直接转化为python类型
- 非结构化数据：HTML
  - 处理方式：正则表达式、xpath、bs4

#### json的数据提取

Python 的 `json` 模块是处理 JSON 数据的核心工具之一，它允许开发者轻松地将 Python 对象序列化为 JSON 格式的字符串或将 JSON 字符串反序列化为 Python 对象。JSON（JavaScript Object Notation）是一种轻量级的数据交换格式，易于阅读和编写，同时也易于机器解析和生成。它广泛应用于 Web 开发、API 通信等领域，成为前后端数据交换的主流格式之一。

Python 的 `json` 模块提供了四个主要函数来处理 JSON 数据：

- `json.dumps(obj, ...)`: 将 Python 对象序列化为 JSON 格式的字符串。
- `json.loads(s, ...)`: 将 JSON 格式的字符串反序列化为 Python 对象。
- `json.dump(obj, fp, ...)`: 将 Python 对象序列化为 JSON 格式并写入文件中。
- `json.load(fp, ...)`: 从文件中读取 JSON 数据并反序列化为 Python 对象。

`indent` 参数用于指定缩进空格数，使得输出更加美观；
`ensure_ascii` 参数用于控制是否转义非 ASCII 字符等

其中类文件对象的理解：
具有read()或者write()方法的对象就是类文件对象，比如f = open(“a.txt”,”r”) f就是类文件对象

```python
import json

data = {
    "name": "John",
    "age": 30,
    "city": "New York"
}

# 序列化为 JSON 字符串并打印
json_string = json.dumps(data, indent=2)
print(json_string)

# 将数据写入 JSON 文件
with open("data.json", "w") as file:
    json.dump(data, file, indent=2)
```

```python
import json

json_string = '{"name": "John", "age": 30, "city": "New York"}'

# 反序列化为 Python 对象并打印
python_object = json.loads(json_string)
print(python_object)

# 从 JSON 文件中读取数据
with open("data.json", "r") as file:
    loaded_data = json.load(file)

print(loaded_data)
```



#### 正则提取数据

[菜鸟教程](https://www.runoob.com/python/python-reg-expressions.html)

Python 的正则表达式（Regular Expression，简称 regex 或 RE）是一种强大的文本处理工具，广泛应用于字符串匹配、搜索、替换等操作中。它允许开发者通过定义特定的模式来描述一类具有共同特征的字符串，并据此执行各种文本处理任务。在 Python 中，正则表达式的功能是通过内置的 `re` 模块提供的

```python
# 导入re模块
import re

# result = re.match(正则表达式,要匹配的字符串)
# 使用match方法进行匹配操作 下面从头开始匹配知道表达式结束，如果匹配失败返回None
result = re.match("ceshi","ceshi.ceshi")
# 如果上一步匹配到数据的话，可以使用group方法来提取数据
info=result.group()
print(info)
```

**运行结果:**

```
ceshi
```

**正则表达式语法元素**：

##### 普通字符

​	没有特殊含义的普通字符，如字母、数字、汉字等。正则表达式中直接表示自己。

##### 元字符

- `.`：匹配除换行符以外的任意单个字符。
- `^`：匹配字符串的开头。
- `$`：匹配字符串的结尾。
- `*`：匹配前面的字符零次或多次。
- `+`：匹配前面的字符一次或多次。
- `?`：匹配前面的字符零次或一次。
- `[]`：定义一个字符类，匹配括号内的任一字符。
- `()`：分组，将一组字符视为一个整体。
- `{m,n}`：匹配前面的字符至少 m 次但不超过 n 次。
- `\d`：匹配任意数字字符，等价于 `[0-9]`。
- `\D`：匹配非数字字符。
- `\s`：匹配空白字符，包括空格、制表符、换行符等。
- `\S`：匹配非空白字符。
- `\w`：匹配字母、数字或下划线，等价于 `[a-zA-Z0-9_]`。
-  `\W`：匹配非字母、非数字、非下划线字符

##### 字符类

字符类用方括号 `[]` 表示，用于匹配括号内的任一字符。例如，`[aeiou]` 匹配任何一个元音字母；`[^aeiou]` 则匹配任何非元音字母。此外，还可以使用范围来简化书写，如 `[a-z]` 匹配小写字母，`[A-Z]` 匹配大写字母

##### 量词

量词用于指定匹配的次数，常见的量词有 `*`、`+`、`?` 和 `{m,n}`。其中，`*` 表示匹配零次或多次；`+` 表示匹配一次或多次；`?` 表示匹配零次或一次；而 `{m,n}` 则允许更加精细地控制匹配次数，例如 `{2,5}` 表示匹配 2 到 5 次

##### 边界

边界符号用于限定匹配的位置，如 `^` 和 `$` 分别表示字符串的开头和结尾。此外，还有单词边界 `\b` 和非单词边界 `\B`，前者匹配单词的开始或结束位置，后者则匹配不在单词边界的任何位置。

##### `re` 模块函数

- `re.match(pattern, string, flags=0)`：尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，`match()` 就返回 `None`。
- `re.search(pattern, string, flags=0)`：扫描整个字符串并返回第一个成功的匹配。
- `re.findall(pattern, string, flags=0)`：找到所有匹配的部分，并以列表形式返回。
- `re.sub(pattern, repl, string, count=0, flags=0)`：将字符串中所有匹配的部分替换为指定的内容。
- `re.split(pattern, string, maxsplit=0, flags=0)`：根据模式分割字符串，返回一个列表

##### 编译正则表达式

为了提高效率，特别是当同一个正则表达式需要被多次使用时，可以先编译正则表达式，然后使用编译后的对象来进行匹配。这可以通过 `re.compile()` 函数实现，它接受一个正则表达式作为参数，并返回一个 `Pattern` 对象。之后，可以调用该对象的方法来进行匹配，如 `pattern.match()`、`pattern.search()` 等。

##### 示例

- **验证邮箱地址**：

```python
import re

def validate_email(email):
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    if re.match(pattern, email):
        print(f"{email} 是一个有效的邮箱地址")
    else:
        print(f"{email} 不是一个有效的邮箱地址")

validate_email("user@example.com")
validate_email("invalid_email@.com")
```

- **提取HTML中的链接**：

```python
import re

html_content = '<a href="https://www.example.com">Visit our website</a>'
links = re.findall(r'href="(.*?)"', html_content)
for link in links:
    print(f"链接: {link}")
```

- **替换文本中的日期格式**：

```python
import re

text = "今天是2024年2月27日，明天是2024-02-28。"
formatted_text = re.sub(r'(\d{4})-(\d{2})-(\d{2})', r'\2/\3/\1', text)
print(f"替换前: {text}")
print(f"替换后: {formatted_text}")
```

- **判断密码强度**：

```python
import re

def check_password_strength(password):
    pattern = r'^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[@$!%*?&])[A-Za-z\d@$!%*?&]{8,}$'
    if re.match(pattern, password):
        print("密码强度符合要求")
    else:
        print("密码强度不够")

check_password_strength("StrongPass123!")
check_password_strength("weakpassword")
```

##### 正则表达式的高级特性

除了上述基本功能外，正则表达式还支持许多高级特性，如非捕获组 `(?:...)`、前瞻断言 `(?=...)` 和后瞻断言 `(?<=...)` 等。这些特性使得正则表达式能够处理更为复杂的情况，同时也增加了表达式的灵活性和表达力。

- **非捕获组**：`(?:...)` 用于分组但不捕获匹配的结果，常用于避免创建不必要的捕获组。
- **前瞻断言**：`(?=...)` 表示匹配后面跟随指定模式的位置，但不消耗字符。
- **后瞻断言**：`(?<=...)` 表示匹配前面紧邻指定模式的位置，同样不消耗字符。

##### 正则表达式实例

字符匹配

| 实例   | 描述           |
| :----- | :------------- |
| python | 匹配 "python". |

字符类

| 实例        | 描述                              |
| :---------- | :-------------------------------- |
| [Pp]ython   | 匹配 "Python" 或 "python"         |
| rub[ye]     | 匹配 "ruby" 或 "rube"             |
| [aeiou]     | 匹配中括号内的任意一个字母        |
| [0-9]       | 匹配任何数字。类似于 [0123456789] |
| [a-z]       | 匹配任何小写字母                  |
| [A-Z]       | 匹配任何大写字母                  |
| [a-zA-Z0-9] | 匹配任何字母及数字                |
| [^aeiou]    | 除了aeiou字母以外的所有字符       |
| [^0-9]      | 匹配除了数字外的字符              |

特殊字符类

| 实例 | 描述                                                         |
| :--- | :----------------------------------------------------------- |
| .    | 匹配除 "\n" 之外的任何单个字符。要匹配包括 '\n' 在内的任何字符，请使用象 '[.\n]' 的模式。 |
| \d   | 匹配一个数字字符。等价于 [0-9]。                             |
| \D   | 匹配一个非数字字符。等价于 [^0-9]。                          |
| \s   | 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。 |
| \S   | 匹配任何非空白字符。等价于 [^ \f\n\r\t\v]。                  |
| \w   | 匹配包括下划线的任何单词字符。等价于'[A-Za-z0-9_]'。         |
| \W   | 匹配任何非单词字符。等价于 '[^A-Za-z0-9_]'。                 |



#### XPath提取数据

lxml是一款高性能的 Python HTML/XML 解析器，我们可以利用XPath，来快速的定位特定元素以及获取节点信息

W3School官方文档：http://www.w3school.com.cn/xpath/index.asp

```xml
<?xml version="1.0" encoding="UTF-8"?>
<bookstore>
    <book>
      <title lang="eng">Harry Potter</title>
      <price>29.99</price>
    </book>

    <book>
      <title lang="eng">Learning XML</title>
      <price>39.95</price>
    </book>
</bookstore>
```

##### 辅助插件

Chrome插件 XPath Helper---国内插件下载地址：https://www.extfans.com/
Firefox插件 XPath Checker

##### xpath语法

下面列出了最有用的表达式：

| 表达式   | 描述                                                       |
| :------- | :--------------------------------------------------------- |
| nodename | 选中该元素及其所有子节点                                   |
| /        | 从根节点选取、或者是元素/子元素的过渡符号                  |
| //       | 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。 |
| .        | 选取当前节点。                                             |
| ..       | 选取当前节点的父节点。                                     |
| @        | 选取属性。                                                 |
| text()   | 选取文本。                                                 |

在下面的表格中，我们已列出了一些路径表达式以及表达式的结果：

| 路径表达式                          | 结果                                                         |
| :---------------------------------- | :----------------------------------------------------------- |
| bookstore                           | 选取所有名为 bookstore 的节点。                              |
| /bookstore                          | 选取根元素 bookstore。注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！ |
| bookstore/book                      | 选取属于 bookstore 的子元素的所有 book 元素。                |
| //book                              | 选取所有 book 子元素，而不管它们在文档中的位置。             |
| bookstore//book                     | 选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。 |
| //@lang                             | 选取名为 lang 的所有属性。                                   |
| /bookstore/book[1]                  | 选取属于 bookstore 子元素的第一个 book 元素。                |
| /bookstore/book[last()]             | 选取属于 bookstore 子元素的最后一个 book 元素。              |
| /bookstore/book[last()-1]           | 选取属于 bookstore 子元素的倒数第二个 book 元素。            |
| /bookstore/book[position()<3]       | 选取最前面的两个属于 bookstore 元素的子元素的 book 元素。    |
| //title[@lang]                      | 选取所有拥有名为 lang 的属性的 title 元素。                  |
| //title[@lang='eng']                | 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。   |
| /bookstore/book[price>35.00]        | 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。 |
| /bookstore/book[price>35.00]//title | 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。 |

xpath基础语法练习：

接下来我们听过豆瓣电影top250的页面来练习上述语法：https://movie.douban.com/top250

- 选择所有的h1下的文本
  - `//h1/text()`
- 获取所有的a标签的href
  - `//a/@href`
- 获取html下的head下的title的文本
  - `/html/head/title/text()`
- 获取html下的head下的link标签的href
  - `/html/head/link/@href`

但是当我们需要选择所有的电影名称的时候会特别费力，通过下一小节的学习，就能够解决这个问题

查找特定的节点

| 路径表达式                              | 结果                                                    |
| :-------------------------------------- | :------------------------------------------------------ |
| //span[@class="s2"]                     | 选择class属性值为s2的所有span标签                       |
| //ul/li[1]                              | 选取属于 ul子标签的第一个 li标签。                      |
| //ul/li[last()]                         | 选取属于 ul子标签的最后一个 li标签。                    |
| //ul/li[last()-1]                       | 选取属于 ul子标签的倒数第二个 li标签。                  |
| //ul/li[position()>1]                   | 选择ul下面的li标签，从第二个开始选择                    |
| //li/span/a[text()='无墟极道']          | 选择所有li下的span标签，仅仅选择文本为 无墟极道 的a标签 |
| //ul/li[position()>8 and position()<12] | 选择ul下面的li标签，从第八个开始选择,第12个结束         |

注意点: 在xpath中，第一个元素的位置是1，最后一个元素的位置是last(),倒数第二个是last()-1



#### BeautifulSoup4提取数据

##### CSS 选择器：BeautifulSoup4的介绍和安装

和 lxml 一样，Beautiful Soup 也是一个HTML/XML的解析器，主要的功能也是如何解析和提取 HTML/XML 数据。

lxml 只会局部遍历，而Beautiful Soup 是基于HTML DOM的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多，所以性能要低于lxml。

BeautifulSoup 用来解析 HTML 比较简单，API非常人性化，支持CSS选择器、Python标准库中的HTML解析器，也支持 lxml 的 XML解析器。

Beautiful Soup 3 目前已经停止开发，推荐现在的项目使用Beautiful Soup 4。使用 pip 安装即可：

```cmd
pip install bs4

# 配置解析器lxml soup = BeautifulSoup(html_doc, 'lxml')。需要如下安装lxml
pip install lxml

#或者使用自带解析器。soup = BeautifulSoup(html_doc, 'html.parser')
```

- **`find(name=None, attrs={}, recursive=True, text=None, \**kwargs)`**：返回匹配的第一个元素。如果不找到任何元素，则返回 `None`。

- **`find_all(name=None, attrs={}, recursive=True, text=None, limit=None, \**kwargs)`**：返回所有匹配的元素列表。如果设置了 `limit` 参数，则只返回指定数量的结果。

- `select(selector)`：根据CSS选择器表达式查找元素，并返回一个包含匹配项的列表。支持类选择器（.）、ID选择器（#）、属性选择器等

  2

  。

官方文档：http://beautifulsoup.readthedocs.io/zh_CN/v4.4.0

##### bs4的基本使用

```python
from bs4 import BeautifulSoup

html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title" name="dromouse"><b>The Dormouse's story</b></p>
<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1"><!-- Elsie --></a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>
<p class="story">...</p>
"""

# 创建 BeautifulSoup 对象，指定使用 'lxml' 作为解析器
soup = BeautifulSoup(html_doc, 'lxml')

# 获取标题内容
print(soup.title.string)

# 获取第一个 <p> 标签的内容
print(soup.p.get_text())

# 获取链接地址
for link in soup.find_all('a'):
    print(link.get('href'))
```



``` python
from bs4 import BeautifulSoup

html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="story">Some links:
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>
"""

# 创建 BeautifulSoup 对象
soup = BeautifulSoup(html_doc, 'lxml')

# 查找所有 <a> 标签并打印 href 属性
for link in soup.find_all('a'):
    print(link.get('href'))

# 查找第一个 <a> 标签并打印 href 属性   .find()查询首个 .find_all()查询所有
a_s=soup.find('a')
print(a_s.title)
print(a_s['href'])
print(a_s)
```



```python
from bs4 import BeautifulSoup
import requests
from lxml import etree

# 获取网页内容
url = "https://example.com"
response = requests.get(url)
html_content = response.text

# 使用 BeautifulSoup 解析 HTML
soup = BeautifulSoup(html_content, 'lxml')

# 将 BeautifulSoup 对象转换为 lxml 的 Element 对象
root = etree.HTML(str(soup))

# 使用 XPath 查询
# 例如，查找所有 <a> 标签的 href 属性
links = root.xpath('//a/@href')
for link in links:
    print(link)

# 或者查找特定 class 的 <div> 标签
divs = root.xpath('//div[@class="specific-class"]')
for div in divs:
    print(etree.tostring(div, encoding='unicode'))
```

| 解析器           | 使用方法                            | 优势                                                        | 劣势                                         |
| :--------------- | :---------------------------------- | :---------------------------------------------------------- | :------------------------------------------- |
| Python标准库     | BeautifulSoup(markup,"html.parser") | Python 的内置标准库、执行速度适中 、文档容错能力强          | Python 2.7.3 or3.2.2) 前的版本中文容错能力差 |
| LXML HTML 解析器 | BeautifulSoup(markup,"lxml")        | 速度快、文档容错能力强                                      | 需要安装 C 语言库                            |
| LXML XML解析器   | BeautifulSoup(markup,"xml")         | 速度快、唯一支持 XML 的解析器                               | 需要安装 C 语言库                            |
| html5lib         | BeautifulSoup(markup,"html5lib")    | 最好的容错性、以浏览器的方式解析文档、生成 HTML5 格式的文档 | 速度慢、不依赖外部扩展                       |

#####  find_all(name, attrs, recursive, text, **kwargs)

###### name 参数

name 参数可以查找所有名字为 name 的tag

传字符串

最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的**标签:**

```python
span = soup.find_all('span')
print(span)

a = soup.find_all('a')
print(a)
```

传正则表达式

如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 match() 来匹配内容.下面例子中找出所有以b开头的标签,这表示和**标签都应该被找到**

```python
import re
for tag in soup.find_all(re.compile("^b")):
    print(tag.name)
# body
```

传列表

如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回.下面代码找到文档中所有标签

```python
data = soup.find_all(['a', 'span'])
print(data)
```

###### 参数

```python
data = soup.find_all(
    name="",	#元素标签名
	attrs={'class':"blue_no_underline"},  #属性匹配
    recursive=Ture ,	#是否递归检索
    text="",	,	#标签文本内容
    
)
print(data)
```

##### find

find的用法与find_all一样，区别在于find返回 第一个符合匹配结果，find_all则返回 所有匹配结果的列表

#####  CSS选择器

这就是另一种与 find_all 方法有异曲同工之妙的查找方法，也是返回所有匹配结果的列表。

- 写 CSS 时，标签名不加任何修饰，类名前加.，id名前加#
- 在这里我们也可以利用类似的方法来筛选元素，用到的方法是 soup.select()，返回类型是 list

######（1）通过标签选择器查找

```python
title = soup.select('title')
print(title)

print(soup.select('a'))

print(soup.select('span'))
```

###### （2）通过类选择器查找

```python
class_data = soup.select('.divcenter')
print(class_data)

# 类名中间带空格处理方式
class_data = soup.select('.adsbygoogle.allinone_good')
print(class_data)
```

###### （3）通过 id 选择器查找

```python
id_data = soup.select('#footercopyright')
print(id_data)
```

###### （4）层级选择器 查找

```python
div_data = soup.select('div .navigationlinknoline')
print(div_data)
```

###### （5）通过属性选择器查找

```python
tr_list = soup.select('tr[align="center"]')
print(tr_list)

a_list = soup.select('a[class="navigationlink"]')
print(a_list)
```

###### （6） 获取文本内容 get_text()

以上的 select 方法返回的结果都是列表形式，可以遍历形式输出，然后用 get_text() 方法来获取它的内容。

```python
title = soup.select('title')
print(title[0].get_text())
print(title[0].getText())
print(title[0].text)
print(title[0].string)

# 伪类选择器
addrs = soup.select('tr td:nth-child(3)')
for addr in addrs:
    print(addr.get_text())
```

###### （7） 获取属性 get('属性的名字')

```python
a_href = soup.select('li a[class="blue_no_underline"]')
# print(a_href)
for a in a_href:
    print(a.get('href'))
```

##### 案例

```python
import os.path

from bs4 import BeautifulSoup
import requests

path = '铃声/'
if not os.path.exists(path):
    os.makedirs(path)

for i in range(1, 2):
    url = 'https://sc.chinaz.com/yinxiao/index_1.html'
    headers = {
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36'
    }
    response = requests.get(url, headers=headers).content.decode('utf-8')

    # 创建 Beautiful Soup 对象
    soup = BeautifulSoup(response, 'lxml')
    div_list = soup.select('#AudioList .container .audio-item')

    for div in div_list:
        mp3_url = "https:" + div.select('audio')[0].get('src')
        title = div.select('.name')[0].get_text().strip()

        res = requests.get(mp3_url, headers=headers)
        with open(path + title + '.mp3', 'wb') as f:
            f.write(res.content)
            print('正在下载{}音乐'.format(title))
```



### 数据存储

大致分为文件存储和数据库存储，本地文本文件存储：txt、json、csv、xls等文件	数据库存储：mysql、mongodb、redis等等
数据库一般远程连接存储，也有常用于本地存储的sqlist关系型数据库。

python中所有open()打开一个文件，文件的打开有很多模式：

- r:以只读方式打开文件,文件的指针将会放在文件的开头,这是默认模式。
- rb:以二进制只读方式打开一个文件，文件指针将会放在文件的开头。
- r+:以读写方式打开一个文件，文件指针将会放在文件的开头。
- rb+: 以二进制读写方式打开一个文件，文件指针将会放在文件的开头。
- w:以写入方式打开一个文件。如果该文件已存在，则将其瞿盖；如果该文件不存在，则创建新文件。
- wb：以二进制写入方式打开一个文件。如果该文件已存在，则将其覆盖；如果该文件不存在，则创建新文件。
- w+：以读写方式打开一个文件。如果该文件已存在，则将其覆盖；如果该文件不存在，则创建新文件。
- wb+：以二进制读写格式打开一个文件。如果该文件已存在，则将其覆盖；如果该文件不存在， 则创建新文件。
- a:以追加方式打开一个文件。如果该文件已存在，文件指针将会放在文件结尾。也就是说，新的内容将会被写入到已有内容之后；如果该文件不存在， 则创建新文件来写入。
- ab:以二进制追加方式打开一个文件。如果该文件已存在，则文件指针将会放在文件结尾。也就是说，新的内容将会被写入到己有内容之后；如果该文件不存在，则创建新文件来写入。
- a+：以读写方式打开一个文件。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式；如果眩文件不存在，则创建新文件来读写。
- ab+：以二进制追加方式打开一个文件。如果该文件已存在，则文件指针将会放在文件结尾；如果该文件不存在，则创建新文件用于读写。

#### 文件写入案例

##### txt写入

```python
import requests
from bs4 import BeautifulSoup

url = 'https://www.zhihu.com/explore'

headers = {
'cookie': '',
'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS 10114) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'

}
html = requests.get(url, headers=headers).text
soup = BeautifulSoup(html, 'lxml')
title_list = soup.select('div .css-1g4zjtl a')
# print(title_list)
for title in title_list:
    print(title.get_text())
    with open('data.txt', 'a', encoding='utf-8')as f:
        f.write(title.get_text() + '\n')
```

##### json写入

| 方法           | 作用                               |
| ------------ | -------------------------------- |
| json.dumps() | 把python对象转换成json对象的一个过程，生成的是字符串。 |
| json.dump()  | 用于将dict类型的数据转成str，并写入到json文件中    |
| json.loads() | 将json字符串解码成python对象              |
| json.load()  | 用于从json文件中读取数据。                  |

```python
import json
import requests
from lxml import etree


url = 'https://www.4399.com/flash/'

response = requests.get(url)
html = etree.HTML(response.content.decode('gbk'))
a_list = html.xpath('//ul[@class="n-game cf"]/li/a')
data_list = []
for a in a_list:
    item = {}
    item['href'] = a.xpath('./@href')[0]
    item['title'] = a.xpath('./b/text()')[0]
    data_list.append(item)

with open('data.json', 'w', encoding='utf-8')as file:
    file.write(json.dumps(data_list))
    # 输出中文
    # file.write(json.dumps(data_list, indent=2, ensure_ascii=False))
```

##### csv写入

```python
import csv 
with open('data.csv', 'w') as csvfile: 
	writer = csv.writer(csvfile) 
	writer.writerow(['id', 'name', 'age']) 
	writer.writerow(['10001', 'Mike', 20]) 
	writer.writerow(['10002', 'Bob', 22]) 
	writer.writerow(['10003', 'Jordan', 21])
    
# 设置列与列之间的分隔符
with open('data.csv', 'w') as csvfile: 
	writer = csv.writer(csvfile, delimiter=' ') 
	writer.writerow(['id', 'name', 'age']) 
	writer.writerow(['10001', 'Mike', 20]) 
	writer.writerow(['10002', 'Bob', 22]) 
	writer.writerow(['10003', 'Jordan', 21])
 	# 多行写入
    writer.writerows([['10001', 'Mike', 20], ['10002', 'Bob', 22], ['10003', 'Jordan', 21]])

# 字典写入
with open('data.csv', 'w') as csvfile: 
	fieldnames = ['id', 'name', 'age'] 
	writer = csv.DictWriter(csvfile, fieldnames=fieldnames) 
	writer.writeheader() 
	writer.writerow({'id': '10001', 'name': 'Mike', 'age': 20}) 
	writer.writerow({'id': '10002', 'name': 'Bob', 'age': 22}) 
```

实操案例：

```python
import csv
import requests

with open('湘江.csv', 'a', encoding='utf-8', newline='')as f:
    fieldnames = ['author', 'basicTitle', 'basicDatetime']
    writer = csv.DictWriter(f, fieldnames=fieldnames)
    writer.writeheader()
    for i in range(1, 3):
        url = 'http://changs.ccgp-hunan.gov.cn/gp/gpcategory/getNotice'
        headers = {
            # 'Host':'changs.ccgp-hunan.gov.cn',
            # 'Cookie':'JSESSIONID=45823044-a192-4910-a217-62f1e148c34c',
            # 'origin': 'http://changs.ccgp-hunan.gov.cn',
            # 'Referer':'http://changs.ccgp-hunan.gov.cn/gp/noticeSerach.html?articleType=2&basicArea=gaoxin',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'
        }
        data = {
            "page": str(i),
            "limit": "10",
            "sidx": "",
            "order": "",
            "categoryId": "184",
            "articleType": "2",
            "projid": "",
            "name": "",
            "cgType": "0",
            "buyerNm": "",
            "buyerOrgNm": "",
            "supplyNm": "",
            "basicType": "1",
            "basicDatetime": "2024-01-01",
            "basicDatetimes": "2024-07-04",
            "type": "0",
            "basicArea": "gaoxin",
            "categoryType": "0"
        }
        response = requests.post(url, headers=headers, data=data)
        print(response.text)
        # print(response.json())

        for res in response.json()['list']['list']:
            item = {}
            item['author'] = res['author']
            item['basicTitle'] = res['basicTitle']
            item['basicDatetime'] = res['basicDatetime']
            writer.writerow(item)
```

##### mysql存储

```python
import requests
import pymysql


class Baidu(object):
    def __init__(self):
        self.db = pymysql.connect(host="localhost", user="root", password="root", db="spiders16")
        self.cursor = self.db.cursor()
        self.url = 'https://talent-holding.alibaba.com/position/search'
        self.headers = {
            'cookie': 'cna=9EQNH8yVwF4CAa8A40LsVZX8; xlly_s=1; XSRF-TOKEN=ce3a8590-8d85-4e2c-9751-4aedd37b3750; prefered-lang=zh; SESSION=RTFENkJFRDhDODRCNEE3RjVGNzZEQUVEN0RBMkYxRDc=; tfstk=fN_-ArtXI-2ktmqhP_ZctWgZQFVceaCzhT5s-pvoAtBA_9Lhq6vhkXBfhwfltajvJtW8T7rU4ipdhtpWEWPFkpBhHpmkEU1Cp66--_YltZFdE6uhp6PyOp6CppjumPfPae8QIpUgS_P6cbC5p3TBcq129QiWNY47se8QI-mmR3z98OjtUurRGSdHtQiCd3gXGKRwde9WAxiX1K9BRe6BcnOyT2gIV39jKxcJKb9KJMdKMj07UX3KJZdbLKC8A4dGrQ3eB_pYCrQx73p1NK3QI4WDLppV5-qAgis5EIWLW-LF3M6B9tMbK35CDtdRERHeKa5lJLf8V85vuLBW2iFs4ICcy6IBD7a5MUpJjZKrO0LRAT7p4ghU_sT53GJwVoy2MabMvd-xh81cMLtOvT2qRLjOGtKGox0M8tSCdBBjegPNSNEu4Av9t0NYMDoeVIvU61OD7zV0jIpgimmEYnAq-0Q3MDiIQ2OvIS4sYD-DM; isg=BLW1a2MzP_L091tO8X_BKJDMxDFvMmlELXWE0jfezC5tDtIA9oZ4FLDIWNI4ToH8',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36'
        }
        self.params = {
            "_csrf": "ce3a8590-8d85-4e2c-9751-4aedd37b3750"
        }


    def get_data(self, page):  # 获取地址和User-Agent
        data = {"channel":"group_official_site","language":"zh","batchId":"","categories":"","deptCodes":[],"key":"","pageIndex":page,"pageSize":10,"regions":"","subCategories":"","shareType":"","shareId":"","myReferralShareCode":""}
        response = requests.post(url=self.url, params=self.params, headers=self.headers, json=data)
        return response.json()

    def parse_data(self, response):
        print(response)
        data_list = response["content"]['datas']
        for node in data_list:
            workLocations = ','.join(node['workLocations'])

            name = node['name']
            requirement = node['requirement']
            self.save_data(workLocations, name, requirement)

    def create_table(self):
        # 使用预处理语句创建表
        sql = '''
                CREATE TABLE IF NOT EXISTS ali(
                    id int primary key auto_increment not null,
                    workLocations VARCHAR(255) NOT NULL, 
                    name VARCHAR(255) NOT NULL, 
                    requirement TEXT)
        '''
        try:
            self.cursor.execute(sql)
            print("CREATE TABLE SUCCESS.")
        except Exception as ex:
            print(f"CREATE TABLE FAILED,CASE:{ex}")

    def save_data(self, workLocations, name, requirement):
        # SQL 插入语句
        sql = 'INSERT INTO ali(id, workLocations, name, requirement) values(%s, %s, %s, %s)'
        # 执行 SQL 语句
        try:
            self.cursor.execute(sql, (0, workLocations, name, requirement))
            # 提交到数据库执行
            self.db.commit()
            print('数据插入成功...')
        except Exception as e:
            print(f'数据插入失败: {e}')
            # 如果发生错误就回滚
            self.db.rollback()

    def run(self):
        self.create_table()
        for i in range(1, 10):
            response = self.get_data(i)
            self.parse_data(response)

        # 关闭数据库连接
        self.db.close()


if __name__ == '__main__':
    baidu = Baidu()
    baidu.run()
```

##### MongoDB

```PYTHON
import pymongo # 如果是云服务的数据库 用公网IP连接
client = pymongo.MongoClient(host='127.0.0.1', port=27017)
collection = client['students']['stu']

# 插入单条数据
student = {'id': '20170101', 'name': 'Jordan', 'age': 20, 'gender': 'male' }
result = collection.insert_one(student)
print(result)

# 插入多条数据
student1 = { 'id': '20170101', 'name': 'Jordan', 'age': 20, 'gender': 'male' }
student2 = { 'id': '20170202', 'name': 'Mike', 'age': 21, 'gender': 'male' }
collection.insert_many([student1, student2])
```

实操案例：

```python
import hashlib

import redis
import requests
import pymongo


class Mgtv(object):
    def __init__(self):
        self.client = pymongo.MongoClient(host='127.0.0.1', port=27017)
        self.collection = self.client['spider']['mgtv']
        self.red = redis.Redis()
        self.headers = {
            'referer': 'https://www.mgtv.com/',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36',
            'x-requested-with': 'XMLHttpRequest'
        }
        self.url = 'https://pianku.api.mgtv.com/rider/list/pcweb/v3'

    def get_md5(self, val):
        """把目标数据进行哈希，用哈希值去重更快"""
        md5 = hashlib.md5()
        md5.update(str(val).encode('utf-8'))
        # print(md5.hexdigest())
        return md5.hexdigest()

    def get_data(self, params):
        response = requests.get(self.url, headers=self.headers, params=params)
        return response.json()

    def parse_data(self, data):
        categoryVideos = data['data']['hitDocs']
        for video in categoryVideos:
            item = {}
            item['story'] = video['story']
            item['subtitle'] = video['subtitle']
            item['title'] = video['title']
            print(item)
            self.save_data(item)

    def save_data(self, item):
        value = self.get_md5(item)
        res = self.red.sadd('mg:filter', value)
        if res:
            self.collection.insert_one(item)
            print('插入成功!!!!')
        else:
            print('数据重复!!!!')

    def main(self):
        for page in range(1, 2):
            params = {
                "allowedRC": "1",
                "platform": "pcweb",
                "channelId": "2",
                "pn": page,
                "pc": "80",
                "hudong": "1",
                "_support": "10000000",
                "kind": "19",
                "area": "10",
                "year": "all",
                "chargeInfo": "a1",
                "sort": "c2"
            }
            data = self.get_data(params)
            self.parse_data(data)


if __name__ == '__main__':
    mg = Mgtv()
    mg.main()

```

#### python使用redis

##### 安装 redis-py

首先，你需要确保已经安装了 Redis 服务，并且可以正常运行。接着，可以通过 pip 安装 redis-py 库：

```bash
pip install redis
```

这将安装最新版本的 redis-py，适用于 Python 3.x 环境。

##### 创建连接

创建与 Redis 的连接有两种方式：直接连接和通过连接池连接。直接连接每次都会创建一个新的连接实例，而连接池则会在程序启动时创建多个连接，并将其保存在一个池中，当需要访问 Redis 时，可以从池中取出一个空闲连接，从而减少频繁建立和断开连接带来的性能损耗。

直接连接

```Python
import redis
#创建一个 Redis 实例
r = redis.Redis(host='localhost', port=6379, db=0, password=None)

# 测试连接是否成功
print(r.ping())  # 如果返回 True，则表示连接成功
```


使用连接池

```Python
import redis
# 创建一个连接池
pool = redis.ConnectionPool(host='localhost', port=6379, db=0, password=None)

# 从连接池中获取一个 Redis 实例
r = redis.Redis(connection_pool=pool)

# 测试连接是否成功
print(r.ping())  # 如果返回 True，则表示连接成功
```

此外，redis-py 还支持通过 URL 方式来构建连接池，URL 格式如下：

redis://[:password]@host:port/db
rediss://[:password]@host:port/db (使用 SSL)
unix://[:password]@/path/to/socket.sock?db=db
例如：

```Python
url = 'redis://:your_password@localhost:6379/0'
pool = redis.ConnectionPool.from_url(url)
r = redis.Redis(connection_pool=pool)
```



##### 基本操作

一旦建立了连接，就可以开始对 Redis 进行读写操作了。以下是几种常见的操作示例：

设置和获取字符串值

```Python
# 设置键 'name' 的值为 'runoob'
r.set('name', 'runoob')

# 获取键 'name' 的值
value = r.get('name')
print(value.decode('utf-8'))  # 输出: runoob
```


为了方便处理字符串编码问题，可以在初始化 Redis 对象时设置 decode_responses=True 参数，这样所有从 Redis 取得的数据都会被自动解码为 Python 字符串，而不是字节串。

```Python
r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
```

批量设置和获取多个键的值

```python
# 批量设置多个键的值
r.mset({'key1': 'value1', 'key2': 'value2'})

# 批量获取多个键的值
values = r.mget(['key1', 'key2'])
print(values)  # 输出: ['value1', 'value2']
```

检查键是否存在

```Python
exists = r.exists('name')
print(exists)  # 如果键存在则输出 1，否则输出 0
```

删除键

```Python
deleted = r.delete('name')
print(deleted)  # 如果删除成功则输出 1，否则输出 0
```

设置带有过期时间的键

```Python
# 设置键 'temp' 的值为 'temporary'，并在 5 秒后自动删除
r.setex('temp', 5, 'temporary')
```

##### 数据类型特定操作

除了简单的字符串操作外，redis-py 还允许你操作其他复杂的数据类型，如哈希表、列表、集合和有序集合。

哈希表操作

```Python
# 向哈希表 'user' 中添加字段 'username' 和 'age'
r.hset('user', 'username', 'john')
r.hset('user', 'age', 30)

# 获取哈希表 'user' 中的所有字段及其对应的值 
user_data = r.hgetall('user')
print(user_data)  # 输出: {'username': 'john', 'age': '30'}
```

列表操作

```Python
# 向列表 'mylist' 的左侧添加元素
r.lpush('mylist', 'first', 'second')

# 向列表 'mylist' 的右侧添加元素
r.rpush('mylist', 'third', 'fourth')

# 获取列表 'mylist' 的所有元素
list_items = r.lrange('mylist', 0, -1)
print(list_items)  # 输出: ['second', 'first', 'third', 'fourth']
```

集合操作

```Python
# 向集合 'myset' 中添加元素
r.sadd('myset', 'apple', 'banana', 'cherry')

# 获取集合 'myset' 中的所有元素
set_items = r.smembers('myset')
print(set_items)  # 输出: {'apple', 'banana', 'cherry'}
```


有序集合操作

```Python
# 向有序集合 'mysortedset' 中添加带分数的成员
r.zadd('mysortedset', {'one': 1.0, 'two': 2.0, 'three': 3.0})

# 获取有序集合 'mysortedset' 中按分数排序的成员
sorted_set_items = r.zrange('mysortedset', 0, -1, withscores=True)
print(sorted_set_items)  # 输出: [('one', 1.0), ('two', 2.0), ('three', 3.0)]
```



##### 管道（Pipeline）

为了提高效率，redis-py 提供了管道机制，允许一次性发送多个命令给 Redis 服务器，并一次性接收所有响应结果。这对于批量操作特别有用，因为它减少了客户端与服务器之间的往返次数8。

```Python
pipe = r.pipeline()
# 在管道中添加多个命令
pipe.set('pipeline_key1', 'value1')
pipe.set('pipeline_key2', 'value2')
pipe.get('pipeline_key1')
pipe.get('pipeline_key2')

# 执行所有命令并获取结果
results = pipe.execute()
print(results)  # 输出: [True, True, 'value1', 'value2']
```



##### 发布/订阅模式

redis-py 也支持 Redis 的发布/订阅功能，这使得你可以轻松地实现消息队列或事件通知系统15。

```Python
深色版本

# 创建发布者
pub = r.pubsub()

# 订阅频道 'channel1'
pub.subscribe('channel1')

# 发布消息到 'channel1'
r.publish('channel1', 'Hello, World!')

# 接收并打印消息
for message in pub.listen():
    if message['type'] == 'message':
        print(message['data'].decode('utf-8'))
        break
```



##### 持久化

虽然 Redis 主要是一个内存数据库，但它也提供了两种持久化选项：RDB 快照和 AOF 日志。对于 Python 开发者来说，通常不需要直接管理这些持久化设置，因为它们是由 Redis 服务器本身控制的。但是，如果你希望在代码中触发持久化操作，比如强制生成 RDB 快照，可以使用以下命令：

```Python
# 触发一次 RDB 快照
r.save()

# 或者让 Redis 在后台执行快照
r.bgsave()
```



##### 错误处理

在实际应用中，可能会遇到网络故障或其他异常情况。redis-py 提供了几种方式来捕获和处理错误。例如，当你尝试连接到不存在的 Redis 服务器时，会抛出 ConnectionError 异常；如果命令格式不正确，则会抛出 ResponseError1。

```Python
try:
    r.set('test', 'value')
except redis.ConnectionError as e:
    print("Failed to connect to Redis:", e)
except redis.ResponseError as e:
    print("Invalid command:", e)
```

综上所述，redis-py 是一个强大且灵活的 Python 客户端库，它不仅简化了与 Redis 交互的过程，还提供了丰富的功能来满足不同的开发需求。通过掌握上述内容，你可以更加高效地利用 Redis 的特性，为你的应用程序增添更多价值。

### 数据去重

防止发出重复的请求
防止存储重复的数据

根据给定的判断依据和给定的去重容器，将原始数据逐一进行判断，判断去重容器中是否有该数据，如果没有那就把该数据对应的判断依据添加去重容器中，同时标记该数据是不重复数据，如果有就不添加，同时标记该数据是重复数据。

#### 临时去重容器和持久化去重容器

1. 临时去重容器

  指如利用list、set等编程语言的数据结构存储去重数据，一旦程序关闭或重启后，去重容器中的数据就被回收了。

  优点：使用与实现简单方便；

  缺点：但无法共享、无法持久化

2. 持久化去重容器

  指如利用 redis、mysql 等数据库存储去重数据。

  优点：持久化、共享；

  缺点：但使用与实现相对复杂

#### 常用几种特殊的原始数据特征值计算

1. 信息摘要 hash 算法（指纹)
2. SimHash 算法 - 模糊文本
3. 布隆过滤器方式 - 上亿级别的数据去重



#### 布隆过滤器

布隆过滤器（Bloom Filter）是一种用于快速检查一个元素是否属于一个集合的数据结构。它的主要特点是高效地判断元素的存在与否，以及节省存储空间。

##### 优点

1. **快速查找元素是否存在于一个集合中**：布隆过滤器可以快速判断一个元素是否存在于一个集合中，而不需要实际存储集合中的元素本身。这是通过使用哈希函数将元素映射到位数组上的不同位置来实现的。如果位数组上的所有对应位置都为1，那么布隆过滤器会认为元素 "可能" 存在于集合中，但也可能是误判。如果有任何一个位置为0，那么元素一定不存在于集合中。这种高效的查询功能使得布隆过滤器在需要快速查找元素的情况下非常有用，比如网络爬虫中的URL去重、拼写检查、黑名单过滤等。
2. **节省存储空间**：相比于使用传统的数据结构（如哈希表或集合）来存储元素，布隆过滤器通常需要更少的存储空间。这是因为它不需要存储元素本身，而只需要存储元素的哈希值和一组二进制位。这使得布隆过滤器在需要存储大量元素但又要求节省内存的情况下非常有用。
3. 布隆过滤器可以用于减轻 Redis 缓存穿透的问题,缓存穿透是指一个恶意用户或恶意请求不断地查询一个缓存中不存在的键，从而导致大量请求直接击穿缓存，访问数据库或其他数据存储，可能会对系统性能和资源造成严重压力
   1. **预过滤缓存键**：在将请求发送到 Redis 之前，可以使用布隆过滤器来快速判断请求的参数是否有效，如果参数被布隆过滤器拦截，那么可以直接拒绝请求，而不用访问 Redis。这可以有效减轻缓存穿透的压力。
   2. **快速判断缓存是否存在**：当布隆过滤器判断请求参数有效时，可以继续访问 Redis 缓存，这时可以更加有信心地判断缓存是否存在，而不用担心缓存穿透问题。

##### 缺点

布隆过滤器也有一定的缺点，主要是存在误判问题。由于多个元素可能映射到相同的位，布隆过滤器可能会错误地认为一个元素存在于集合中，而实际上它并不存在。因此，在使用布隆过滤器时需要权衡误判率和存储空间的需求，根据具体应用场景进行选择和调优。可以理解为告诉你 “某样东西一定不存在或者可能存在”

##### 实现代码

```bash
pip install pybloom_live
```

```python
import hashlib

class BloomFilter:
    def __init__(self, size, num_hashes):
        self.size = size
        self.num_hashes = num_hashes
        self.bit_array = [0] * size

    def add(self, element):
        for i in range(self.num_hashes):
            hash_value = int(hashlib.md5(f"{element}{i}".encode()).hexdigest(), 16) % self.size
            self.bit_array[hash_value] = 1

    def contains(self, element):
        for i in range(self.num_hashes):
            hash_value = int(hashlib.md5(f"{element}{i}".encode()).hexdigest(), 16) % self.size
            if self.bit_array[hash_value] == 0:
                return False
        return True

aa = BloomFilter(5, 2)
aa.add('567')

print(aa.contains('456'))
print(aa.bit_array)
```



```python
from pybloom_live import BloomFilter

visited_urls = BloomFilter(capacity=1000, error_rate=0.001)

list1 = [1,2,3,4,123,345,123,345,12,23,345,123,354,34556,4,567567,234,123,345,34,123,12]
for i in list1:
    print(visited_urls.add(i))
```







