# Scrapy采集框架

## 简介

为什么学习scrapy？

1. scrapy不能解决剩下的10%的爬虫需求
2. 能够让开发过程方便、快速
3. scrapy框架能够让我们的爬虫效率更高

**Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架，我们只需要实现少量的代码，就能够快速的抓取。**

**实现流程：**![](.\image\image-20241210105014584.png)

**其流程可以描述如下：**

1. 调度器把requests-->引擎-->下载中间件--->下载器
2. 下载器发送请求，获取响应---->下载中间件---->引擎--->爬虫中间件--->爬虫
3. 爬虫提取url地址，组装成request对象---->爬虫中间件--->引擎--->调度器
4. 爬虫提取数据--->引擎--->管道
5. 管道进行数据的处理和保存

**注意：**

- 图中绿色线条的表示数据的传递
- 注意图中中间件的位置，决定了其作用
- 注意其中引擎的位置，所有的模块之前相互独立，只和引擎进行交互

**模块介绍：**

![image-20241210115423152](.\image\image-20241210115423152.png)



## 入门使用

### 1 scrapy项目实现流程

1. 创建一个scrapy项目:scrapy startproject mySpider
2. 生成一个爬虫:scrapy genspider douban movie.douban.com
3. 提取数据:完善spider，使用xpath等方法
4. 保存数据:pipeline中保存数据

### 2 创建scrapy项目

> 下面以抓取豆瓣top250来学习scrapy的入门使用：https://movie.douban.com/top250

安装scrapy命令：sudo apt-get install scrapyscrapy==2.5.0 或者：pip install scrapy==2.5.0

#### 1.创建项目面板

```python
scrapy 2.5.0 - no active project

usage:
  scrapy <command>[options] [args]

Available commands :
  bench      Run quick benchmark test #测试电脑性能
  fetch      Fetch a URL using the scrapy down1oader#将源代码下载下来并显示出来
  genspider      Generate new spider using pre-defined temp1ates#创建一个新的spider文件
  runspider      Run a self-contained spider (without creating a project)# 这个和通过craw1启动爬虫不同，scrapy runspider爬虫文件名称
  settings      Get settings values#获取当前的配置信息
  she11      Interactive scraping console#进入scrapy 的交互模式
  startproject      create new project#创建爬虫项目
  version      Print scrapy version#显示scrapy框架的版本
  view      open URL in browser，as seen by scrapy#将网页document内容下载下来，并且在浏览器显示出来
```

创建scrapy项目的命令：scrapy startproject +<项目名字>

示例：scrapy startproject myspider

生成的目录和文件结果如下：

![image-20241210144626529](.\image\image-20241210144626529.png)

### 3 创建爬虫

命令：**在项目路径下执行**:scrapy genspider +<爬虫名字> + <允许爬取的域名>

示例：

```
cd myspider
scrapy genspider douban movie.douban.com
```

生成的目录和文件结果如下：

![企业微信截图_17338138383323](D:\my_spider_mk\image\image-20241210153323433.png)

### 4 完善spider

完善spider即通过方法进行数据的提取等操作

在\myspider\myspider\douban.py中修改内容如下:

```python
import scrapy
from scrapy.http import HtmlResponse
# 自定义spider类，继承scrapy.spider
class DoubanSpider(scrapy.Spider):
    # 爬虫名字
    name = 'douban'
    # 允许爬取的范围，防止爬虫爬到别的网站
    allowed_domains = ['douban.com']
    # 开始爬取的url地址
    start_urls = ['https://movie.douban.com/top250']

    # 数据提取的方法，接受下载中间件传过来的response
    def parse(self, response:HtmlResponse, **kwargs):
        # scrapy的response对象可以直接进行xpath
        ol_list = response.xpath('//ol[@class="grid_view"]/li')
        print(ol_list)
        for ol in ol_list:
            # 创建一个数据字典
            item = {}
            # 利用scrapy封装好的xpath选择器定位元素，并通过extract()或extract_first()来获取结果
            item['title'] = ol.xpath('.//div[@class="hd"]/a/span[1]/text()').extract_first()
            item['rating'] = ol.xpath('.//div[@class="bd"]/div/span[2]/text()').extract_first()
            item['quote'] = ol.xpath('.//div[@class="bd"]//p[@class="quote"]/span/text()').extract_first()
            print(item)
```

**注意：**

1. response.xpath方法的返回结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有一些额外的方法
2. extract() 返回一个包含有字符串的列表
3. extract_first() 返回列表中的第一个字符串，列表为空没有返回None
4. spider中的parse方法必须有
5. 需要抓取的url地址必须属于allowed_domains域名下；start_urls中的url地址没有这个限制，只是一个开始的页
6. 启动爬虫的时候注意启动的位置，是在项目路径下启动

#### 4.1 response对象属性

```python
print(response.url)	#HTTP相应的 URL地址，str类型的
print(response.status)	#HTTP响应状态码，int类型的（在pycharm的控制台中你可以看到，例如200,404）
print(response.body) 	#body HTTP响应正文，bytes类型，（即网页的 HTML 源代码）
print(response.text)	#text 文本形式的HTTP响应正文，str类型，由response.body使用response.encoding解码得到
print(response.encoding)  # 查看当前编码,HTTP响应正文的编码
response.encoding = 'gbk'  # 修改编码
print(response.text)  # 使用新的编码解码文本

print(response.request)	#产生该HTTP响应的Requset对象
print(response.request.url)  # 产生该HTTP响应的Requset对象的请求的 URL

print(response.headers)	# 返回响应的 HTTP 头信息，以字典形式存储。
print(response.headers.get('Content-Type'))  # 获取 Content-Type 头

print(response.flags)	#返回与响应关联的标志列表，通常用于标记响应的特殊状态（如重定向）。类型list

response.css()	#返回值scrapy.selector.SelectorList，使用 CSS 选择器解析 HTML 内容，并返回一个 SelectorList 对象，包含所有匹配的元素。
titles = response.css('h2 a::text').getall()  # 获取所有 h2 标签下的 a 标签文本

response.xpath()	# 返回值scrapy.selector.SelectorList，使用 XPath 选择器解析 HTML 内容，并返回一个 SelectorList 对象，包含所有匹配的元素。
titles = response.xpath('//h2/a/text()').getall()  # 获取所有 h2 标签下的 a 标签文本
print(titles)

# response.follow()根据相对或绝对 URL 构建一个新的 Request 对象，并自动处理重定向和基础 URL。返回值scrapy.http.Request
# 参数：
# url：要跟随的链接，可以是相对 URL 或绝对 URL。
# callback：回调函数，用于处理新请求的响应。
# meta：传递给回调函数的元数据。
next_page = response.css('a.next::attr(href)').get()
if next_page:
    yield response.follow(next_page, self.parse)  # 跟随下一页链接

# response.follow_all()根据多个相对或绝对 URL 构建多个 Request 对象，并自动处理重定向和基础 URL。返回值list[scrapy.http.Request]
# 参数：
# urls：要跟随的链接列表，可以是相对 URL 或绝对 URL。
# callback：回调函数，用于处理新请求的响应。
# meta：传递给回调函数的元数据。
links = response.css('a.page-num::attr(href)').getall()
for link in response.follow_all(links, self.parse_page):
    yield link  # 跟随多个页面链接

# 将相对url转为绝对url。返回值str（绝对URL）
relative_url = "page/2"
absolute_url = response.urljoin(relative_url)
print(absolute_url)



# ===选择器操作：

# 从 SelectorList 中获取第一个匹配的元素的文本或属性值 str。如果没有任何匹配项，则返回 None。
title = response.css('h1::text').get()  # 获取 h1 标签的文本
print(title)

# 从 SelectorList 中获取所有匹配的元素的文本或属性值，返回一个列表。list[str]
titles = response.css('h2 a::text').getall()  # 获取所有 h2 标签下的 a 标签文本
print(titles)

# 获取匹配元素的属性字典。适用于单个元素的选择器。返回值dict
link = response.css('a.next').attrib['href']  # 获取 a 标签的 href 属性
print(link)

# 使用正则表达式从匹配的元素中提取文本。返回所有匹配的结果列表。返回值list[str]
prices = response.css('.price::text').re(r'\d+')  # 提取价格中的数字
print(prices)

# 使用正则表达式从匹配的元素中提取第一个匹配的文本。如果没有匹配项，则返回 None。返回str 或 None
price = response.css('.price::text').re_first(r'\d+')  # 提取第一个价格中的数字
print(price)



# ===其他常用方法：

# 获取传递给当前响应的 Request 对象的元数据。元数据可以在请求时通过 meta 参数传递给回调函数。
def parse(self, response):
    item = response.meta.get('item')  # 获取传递的元数据
    print(item)

# 创建当前 Response 对象的浅拷贝。如果你需要在不同的回调函数中使用相同的响应对象，可以使用此方法。返回值：scrapy.http.Response
copied_response = response.copy()

response.errback
# 设置错误回调函数，用于处理请求过程中发生的错误（如网络超时、DNS 解析失败等）。错误回调函数会接收一个 Twisted Failure 对象作为参数。
# 错误回调函数。
def parse_error(self, failure):
    self.logger.error(f"Error: {failure}")

def start_requests(self):
    yield scrapy.Request(
        url='https://example.com',
        callback=self.parse,
        errback=self.parse_error
    )
```



### 5 利用管道pipeline来处理(保存)数据

#### 5.1 对douban爬虫进行修改完善

在爬虫文件douban.py中parse()函数中最后添加

```
yield item
```

**注意：yield能够传递的对象只能是：BaseItem,Request,dict,None**

#### 5.2 修改pipelines.py文件

```python
class MyspiderPipeline:
    # 爬虫开启时打开一个文件，用于存储爬取到的数据。（爬虫开启时，就会运行该函数）
    def open_spider(self, spider):
        self.file = open('items.json', 'a',encoding="utf-8")
    
    # 关闭爬虫时关闭文件。（爬虫关闭时，就会运行该函数）
    def close_spider(self, spider):
        self.file.close()
    
    # @classmethod
    # def from_crawler(cls, crawler):
    #     settings = crawler.settings
    #     return cls(settings)

    # 爬虫文件中提取数据的方法每yield一次item，就会运行一次
    # 该方法为固定名称函数
    # item：抓取到的项目，通常是一个 Python 字典或自定义的 Item 对象。
    # spider：发送该项目的爬虫对象，可以通过它访问爬虫的属性和方法。
    # 返回值：
		# 返回处理后的项目，以便传递给下一个管道（如果有）。
		# 如果你不想将项目传递给下一个管道，可以直接返回 None。
		# 如果你想丢弃该项目，可以抛出 DropItem 异常。
    def process_item(self, item, spider):
        # 这里自定义的存储到文件，也可以自定义其他存储
        line = json.dumps(dict(item), ensure_ascii=False) + "\n"
        self.file.write(line)
        # 这里将数据返回，有scrapy自带的存储到文件的方法。下面有使用
        return item
```

#### 5.3 在settings.py设置开启pipeline

```python
# 数值越小，优先级越高。
ITEM_PIPELINES = {
   'myspider.pipelines.MyspiderPipeline': 300,
}

# 不尊搜robots.txt
# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# 带上请求头
# Override the default request headers:
DEFAULT_REQUEST_HEADERS = {
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en',
'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'
}
```

### 6 运行scrapy

#### 6.1命令运行

命令：在项目目录下执行scrapy crawl +<爬虫名字>

示例：scrapy crawl douban --nolog

#### 6.2 debug运行

```python
# scrapy封装的命令执行
from scrapy import cmdline

if __name__ == '__main__':
    # 解析的是一个列表对象
    # 获取的json文件会乱码 需要修改配置 FEED_EXPORT_ENCODING = 'utf-8'
    cmdline.execute('scrapy crawl douban -o douban.json -s FEED_EXPORT_ENCODING="utf-8"'.split())
```

**注意：**

- 安装2.5版本之后运行代码可能会遇到以下错误

```python
AttributeError: module 'OpenSSL.SSL' has no attribute 'SSLv3_METHOD'
ImportError: cannot import name 'HTTPClientFactory' from 'twisted.web.client' (unknown location)
```

- 降低OpenSSL和cryptography以及Twisted的版本

```python
pip install pyOpenSSL==22.0.0
pip install cryptography==38.0.4
pip install Twisted==20.3.0
# 或者
pip install cryptography==3.4.8
pip install pyOpenSSL==21.0.0
```

- 异步报错

```python
TypeError: ProactorEventLoop is not supported, got: <ProactorEventLoop running=False closed=False debug=False>
```

- 这个错误通常表示正在尝试在不支持 `ProactorEventLoop` 的操作系统上运行Scrapy。Scrapy目前不支持Windows上的`ProactorEventLoop`，因此您需要将其更改为`SelectorEventLoop`。
- 可以通过在Scrapy项目的settings.py文件中添加以下代码来更改默认的事件循环

```python
import asyncio
if hasattr(asyncio, "WindowsSelectorEventLoopPolicy"):
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
```

- 2.5的版本适配的是3.7的py版本,需要找到你解释器对应的版本信息
- 可以运行尝试

### 7 输出的数据日志信息

#### 7.1 日志等级

- CRITICAL：严重错误
- ERROR：一般错误
- WARNING：警告
- INFO: 一般信息
- DEBUG：调试信息

**注意：** 默认的日志等级是DEBUG

#### 7.2日志输出信息

```txt
Versions:使用的工具版本信息
Overridden settings： 重写的配置
Telnet Password：Telnet 平台密码（Scrapy附带一个内置的telnet控制台，用于检查和控制Scrapy运行过程）
Enabled extensions ：开启的拓展功能
Enabled downloader middlewares：开启的下载器中间件
Enabled spider middlewares：开启的爬虫中间件
Enabled item pipelines：开启的管道
Dumping Scrapy stats：所以的信息汇总
```

#### 7.3 日志等级设置

- 修改settings.py文件

```python
LOG_LEVEL = 'WARNING' # 设置日志显示的等级为WARNING，DEBUG输出所有
LOG_FILE = './log.txt' # 将日志信息全部记录到log.txt文件中
```

## 三、scrapy发送翻页请求

### 1 翻页请求的思路

对于要提取如下图中所有页面上的数据该怎么办？

回顾requests模块是如何实现翻页请求的：

1. 找到下一页的URL地址
2. 调用requests.get(url)

scrapy实现翻页的思路：

1. 找到下一页的url地址
2. 构造url地址的请求，传递给引擎

### 2 scrapy实现翻页请求

#### 2.1 实现方法

1. 确定url地址
2. 构造请求，scrapy.Request(url,callback)
   - callback：指定解析函数名称，表示该请求返回的响应使用哪一个函数进行解析
3. 把请求交给引擎：yield scrapy.Request(url,callback)

#### 2.2 豆瓣爬虫

> 通过爬取豆瓣top250页面的电影信息,学习如何实现翻页请求
>
> 地址：https://movie.douban.com/top250

##### 思路分析：

1. 获取首页的数据
2. 寻找下一页的地址，进行翻页，获取数据

##### 注意：

1. 可以在settings中设置ROBOTS协议

   ```
   # False表示忽略网站的robots.txt协议，默认为True
   ROBOTSTXT_OBEY = False
   ```

2. 可以在settings中设置User-Agent：

   ```
   # scrapy发送的每一个请求的默认UA都是设置的这个User-Agent
   USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'
   ```

#### 2.3 代码实现

在爬虫文件的parse方法中：

```python
    def parse(self, response, **kwargs):
        # scrapy的response对象可以直接进行xpath
        ol_list=response.xpath("//ol[@class='grid_view']/li")

        for ol in ol_list:
            item={}
            # 取豆瓣信息ban中电影标题、评分、短评
            item['title']=ol.xpath(".//div[@class='hd']/a/span[1]/text()").extract_first()
            item['rating']=ol.xpath(".//div[@class='bd']/div/span[2]/text()").extract_first()
            item['quote']=ol.xpath(".//div[@class='bd']//p[@class='quote']/span/text()").extract_first()
            item["xinxi"]="信息"
            yield item

        # 翻页
        next_page = response.xpath("//span[@class='next']/a/@href").extract_first()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)
        
```

#### 2.4 scrapy.Request的更多参数

```python
scrapy.Request(
    url
    [
     ,callback=self.函数名		# url的响应交给【函数名】去处理
     ,method="GET"		# 指定请求
     ,headers={}		# 请求头
     ,body={}		# 请求体
     ,cookies={}		# 请求cookies
     ,meta={}		# 用于往处理函数中传入参数，
        		    # 取用extra_param = response.meta.get('extra_param')
     ,dont_filter=False		#默认为False，会过滤请求的url地址，即请求过的url地址不会继续被请求，对需要重复请求的url地址可以把它设置为Ture，比如贴吧的翻页请求，页面的数据总是在变化;start_urls中的地址会被反复请求，否则程序不会启动
    ]
)
```



#### 2.5 重写start_requests方法

上述方法可以帮助我们实现翻页的问题,但是这种翻页并不是框架的最优解,我们可以重写Spider的start_requests方法,自己生成对应的请求网址给到引擎进行调度

```python
    def start_requests(self):
        for i in range(0, 10):
            url = 'https://movie.douban.com/top250?start={}&filter='.format(i * 25)
            yield scrapy.Request(url)
```

### 3 meta参数的使用

##### meta的形式:字典

##### meta的作用：meta可以实现数据在不同的解析函数中的传递

在爬虫文件的parse方法中，提取详情页增加之前callback指定的parse_detail函数：

```python
def parse(self,response):
    ...
    yield scrapy.Request(detail_url, callback=self.parse_detail,meta={"item":item})
...

def parse_detail(self,response):
    #获取之前传入的item
    item = resposne.meta["item"]
```

##### 特别注意

1. meta参数是一个字典
2. meta字典中有一个固定的键`proxy`，表示代理ip，关于代理ip的使用我们将在scrapy的下载中间件的学习中进行介绍
3. meta的download_timeout设置请求超时

### 4 item的使用

#### 4.1 Item能够做什么

1. 定义item即提前规划好哪些字段需要抓取，scrapy.Field()仅仅是提前占坑，通过item.py能够让别人清楚自己的爬虫是在抓取什么，同时定义好哪些字段是需要抓取的，没有定义的字段不能使用，防止手误
2. 在python大多数框架中，大多数框架都会自定义自己的数据类型(在python自带的数据结构基础上进行封装)，目的是增加功能，增加自定义异常

#### 4.2 定义Item

在items.py文件中定义要提取的字段：

```
class MyspiderItem(scrapy.Item):
    title = scrapy.Field()  # 标题
    rating = scrapy.Field()  # 评估
    quote = scrapy.Field()  # 概述
```

#### 4.3 使用Item

Item使用之前需要先导入并且实例化，之后的使用方法和使用字典相同

修改爬虫文件douban.py：

```python
from myspider.items import MyspiderItem  # 导入Item，注意路径
...
    def parse(self, response):
        # print(response.meta['data'])
        ol_list = response.xpath('//ol[@class="grid_view"]/li')

        for ol in ol_list:
            item = MyspiderItem()
            item['title'] = ol.xpath('.//div[@class="hd"]/a/span[1]/text()').extract_first()
            item['rating'] = ol.xpath('.//div[@class="bd"]/div/span[2]/text()').extract_first()
            item['quote'] = ol.xpath('.//div[@class="bd"]//p[@class="quote"]/span/text()').extract_first()
            # item['yeshu'] = response.meta['data']
            # print(item)
            yield item
```

##### 注意：

1. from myspider.items import MyspiderItem这一行代码中 注意item的正确导入路径，忽略pycharm标记的错误
2. python中的导入路径要诀：从哪里开始运行，就从哪里开始导入
3. 不存在的字段写入会抛出异常！！！！

## 四、scrapy的管道使用

### 1. 了解scrapyShell

 scrapy shell是scrapy提供的一个终端工具，能够通过它查看scrapy中对象的属性和方法，以及测试xpath

使用方法：

```python
scrapy shell https://movie.douban.com/top250
```

在终端输入上述命令后，能够进入python的交互式终端，此时可以使用：

- response.xpath()：直接测试xpath规则是否正确
- response.url：当前响应的url地址
- response.request.url：当前响应对应的请求的url地址
- response.headers：响应头
- response.body：响应体，也就是html代码，默认是byte类型
- response.requests.headers：当前响应的请求头

### 3 settings.py中的设置信息

##### 3.1 为什么项目中需要配置文件

 在配置文件中存放一些公共变量，在后续的项目中方便修改，如：本地测试数据库和部署服务器的数据库不一致

##### 3.2 配置文件中的变量使用方法

1. 变量名一般全部大写
2. 导入即可使用

##### 3.3 settings.py中的重点字段和含义

- USER_AGENT 设置ua
- ROBOTSTXT_OBEY 是否遵守robots协议，默认是遵守
- CONCURRENT_REQUESTS 设置并发请求的数量，默认是16个
- DOWNLOAD_DELAY 下载延迟，默认无延迟
- COOKIES_ENABLED 是否开启cookie，即每次请求带上前一次的cookie，默认是开启的
- DEFAULT_REQUEST_HEADERS 设置默认请求头
- SPIDER_MIDDLEWARES 爬虫中间件，设置过程和管道相同
- DOWNLOADER_MIDDLEWARES 下载中间件
- LOG_LEVEL 控制终端输出信息的log级别，终端默认显示的是debug级别的log信息
  - LOG_LEVEL = "WARNING"
- LOG_FILE 设置log日志文件的保存路径，如果设置该参数，终端将不再显示信息
  - LOG_FILE = "./test.log"
- 其他设置参考：https://www.jianshu.com/p/df9c0d1e9087

### 4 pipeline管道的深入使用

##### 4.1 pipeline中常用的方法：

1. process_item(self,item,spider):实现对item数据的处理
2. open_spider(self, spider): 在爬虫开启的时候仅执行一次
3. close_spider(self, spider): 在爬虫关闭的时候仅执行一次

#### 4.2 管道文件的修改

在pipelines.py代码中完善

```python
import pymysql
import pymongo


class MyspiderMySQLPipeline:
    def open_spider(self, spider):
        # 判断是哪个爬虫  名字不同可能执行的爬虫项目也不同
        if spider.name == 'douban':
            self.db = pymysql.connect(host="localhost", user="root", password="root", db="spiders9")
            self.cursor = self.db.cursor()
            # 创建变语法
            sql = '''
                                CREATE TABLE IF NOT EXISTS douban(
                                    id int primary key auto_increment not null,
                                    quote VARCHAR(255) NOT NULL, 
                                    rating VARCHAR(255) NOT NULL, 
                                    title VARCHAR(255) NOT NULL)
                                '''
            try:
                self.cursor.execute(sql)
                print("CREATE TABLE SUCCESS.")
            except Exception as ex:
                print(f"CREATE TABLE FAILED,CASE:{ex}")

    def process_item(self, item, spider):
        # SQL 插入语句
        sql = 'INSERT INTO douban(id, quote, rating, title) values(%s, %s, %s, %s)'
        # 执行 SQL 语句
        try:
            self.cursor.execute(sql, (0, item['quote'], item['rating'], item['title']))
            # 提交到数据库执行
            self.db.commit()
            print('mysql数据插入成功...')
        except Exception as e:
            print(f'数据插入失败: {e}')
            # 如果发生错误就回滚
            self.db.rollback()
        # 不return的情况下，另一个权重较低的pipeline将不会获得item,否则后一个pipeline取到的数据为None值
        return item 

    def close_spider(self, spider):  # 在爬虫关闭的时候仅执行一次
        # 关闭文件
        if spider.name == 'douban':
            self.db.close()


class MyspiderMongoDBPipeline:
    def open_spider(self, spider):  # 在爬虫开启的时候仅执行一次
        if spider.name == 'douban':
            con = pymongo.MongoClient(host='127.0.0.1', port=27017)  # 实例化mongoclient
            self.collection = con.spiders9.douban  # 创建数据库名为spiders9,集合名为douban的集合操作对象

    def process_item(self, item, spider):
        if spider.name == 'douban':
            print('mongo保存成功')
            self.collection.insert_one(dict(item))  # 此时item对象需要先转换为字典,再插入
        # 不return的情况下，另一个权重较低的pipeline将不会获得item
        return item
```

#### 4.3 开启管道

在settings.py设置开启pipeline

```python
ITEM_PIPELINES = {
   'myspider.pipelines.MyspiderMySQLPipeline': 300,
   'myspider.pipelines.MyspiderMongoDBPipeline': 400,
}
```

**思考：pipeline在settings中能够开启多个，为什么需要开启多个？**

1. 不同的pipeline可以处理不同爬虫的数据，通过spider.name属性来区分
2. 不同的pipeline能够对一个或多个爬虫进行不同的数据处理的操作，比如一个进行数据清洗，一个进行数据的保存
3. 同一个管道类也可以处理不同爬虫的数据，通过spider.name属性来区分

#### 4.4 pipeline使用注意点

1. 使用之前需要在settings中开启
2. pipeline在setting中键表示位置(即pipeline在项目中的位置可以自定义)，值表示距离引擎的远近，越近数据会越先经过
3. 有多个pipeline的时候，process_item的方法必须return item,否则后一个pipeline取到的数据为None值
4. pipeline中process_item的方法必须有，否则item没有办法接受和处理
5. process_item方法接受item和spider，其中spider表示当前传递item过来的spider
6. open_spider(spider) :能够在爬虫开启的时候执行一次
7. close_spider(spider) :能够在爬虫关闭的时候执行一次
8. 上述俩个方法经常用于爬虫和数据库的交互，在爬虫开启的时候建立和数据库的连接，在爬虫关闭的时候断开和数据库的连接

## 五、中间件

middlewares.py文件

```python
from scrapy import signals

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter


class MyspiderSpiderMiddleware:
    """
        蜘蛛中间件（Spider Middleware）主要用于处理从下载器传递给蜘蛛的响应，以及从蜘蛛传递给引擎的结果（如请求或项目）。它可以在响应进入蜘蛛之前对其进行处理，或者在蜘蛛返回结果之后对其进行处理。
    """


    @classmethod
    def from_crawler(cls, crawler):
        """
            这是一个类方法，Scrapy 会调用这个方法来创建中间件实例。它接收一个 crawler 对象作为参数，该对象包含了爬虫的所有配置和组件。
            在这个方法中，我们创建了一个中间件实例 s，并将其与 spider_opened 信号连接起来。当爬虫启动时，spider_opened 方法会被调用。
        """

        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_spider_input(self, response, spider):
        """
            当响应从下载器传递给蜘蛛时，Scrapy 会调用这个方法。你可以在这里对响应进行预处理，例如检查响应的状态码、解析响应内容等。
            如果你在这个方法中返回 None，Scrapy 会继续将响应传递给蜘蛛的回调函数。如果你抛出异常，Scrapy 会调用 process_spider_exception 方法来处理异常。
        """

        return None

    def process_spider_output(self, response, result, spider):
        """
            当蜘蛛处理完响应并返回结果（如新的请求或项目）时，Scrapy 会调用这个方法。你可以在这里对蜘蛛返回的结果进行处理，例如过滤请求、修改项目等。
            你必须返回一个可迭代的对象，其中包含 Request 或 Item 对象。
        """

        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        """
            当蜘蛛或 process_spider_input 方法抛出异常时，Scrapy 会调用这个方法。你可以在这里处理异常，例如记录日志、重试请求等。
            你可以返回 None 继续处理异常，或者返回一个新的 Response 或 Request 对象来终止异常处理链。
        """

        pass

    def process_start_requests(self, start_requests, spider):
        """
            当爬虫启动时，Scrapy 会调用这个方法来处理爬虫的起始请求。你可以在这里修改起始请求，例如添加额外的请求头、更改 URL 等。
            你必须返回一个只包含 Request 对象的可迭代对象。
        """

        for r in start_requests:
            yield r

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)


class MyspiderDownloaderMiddleware:
	"""
        下载器中间件（Downloader Middleware）主要用于处理从引擎发送到下载器的请求，以及从下载器返回的响应。它可以在请求发送之前对其进行修改，或者在响应返回之后对其进行处理。
	"""

    @classmethod
    def from_crawler(cls, crawler):
        """
        	如上效果类似
        """

        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        """
            当请求从引擎发送到下载器时，Scrapy 会调用这个方法。你可以在这里对请求进行预处理，例如添加代理、修改请求头、设置超时等。
            你可以返回 None 继续处理请求，或者返回一个新的 Response 对象（模拟响应），或者返回一个新的 Request 对象（重定向请求），或者抛出 IgnoreRequest 异常来忽略请求。
        """
        # Called for each request that goes through the downloader
        # middleware.

        # Must either:
        # - return None: continue processing this request
        # - or return a Response object
        # - or return a Request object
        # - or raise IgnoreRequest: process_exception() methods of
        #   installed downloader middleware will be called
        return None

    def process_response(self, request, response, spider):
        """
			当响应从下载器返回时，Scrapy 会调用这个方法。你可以在这里对响应进行后处理，例如修改响应内容、检查响应状态码等。
			你必须返回一个 Response 对象，或者返回一个新的 Request 对象（重定向请求），或者抛出 IgnoreRequest 异常来忽略响应。
        """
        # Called with the response returned from the downloader.

        # Must either;
        # - return a Response object
        # - return a Request object
        # - or raise IgnoreRequest
        return response

    def process_exception(self, request, exception, spider):
        """
            当下载器或 process_request 方法抛出异常时，Scrapy 会调用这个方法。你可以在这里处理异常，例如记录日志、重试请求等。
            你可以返回 None 继续处理异常，或者返回一个新的 Response 对象（模拟响应），或者返回一个新的 Request 对象（重试请求），或者抛出 IgnoreRequest 异常来忽略异常。
        """
        # Called when a download handler or a process_request()
        # (from other downloader middleware) raises an exception.

        # Must either:
        # - return None: continue processing this exception
        # - return a Response object: stops process_exception() chain
        # - return a Request object: stops process_exception() chain
        pass

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)

```

settings.py配置文件中需要启用并配置优先级

```python
# settings.py

# 启用蜘蛛中间件
SPIDER_MIDDLEWARES = {
    'mySpider.middlewares.MyspiderSpiderMiddleware': 543,
}

# 启用下载器中间件
DOWNLOADER_MIDDLEWARES = {
    'mySpider.middlewares.MyspiderDownloaderMiddleware': 543,
}
```

### 内置中间件

Scrapy 提供了一些内置的中间件，它们可以帮助你处理常见的任务，例如身份验证、重试、重定向、压缩等。你可以在 `settings.py` 中启用或禁用这些内置中间件。

- **`HttpAuthMiddleware`**：用于处理 HTTP 基本身份验证。
- **`DownloadTimeoutMiddleware`**：用于设置请求的超时时间。
- **`DefaultHeadersMiddleware`**：用于设置默认的请求头。
- **`UserAgentMiddleware`**：用于设置用户代理（User-Agent）。
- **`RetryMiddleware`**：用于自动重试失败的请求。
- **`RedirectMiddleware`**：用于处理 HTTP 重定向。
- **`HttpCompressionMiddleware`**：用于处理 Gzip 压缩的响应。
- **`CookiesMiddleware`**：用于管理 cookies。
- **`HttpProxyMiddleware`**：用于通过代理服务器发送请求。

### 中间件的优先级

中间件的优先级决定了它们的执行顺序。优先级值越小，中间件越早执行。你可以通过调整优先级值来控制中间件的执行顺序。通常，优先级范围为 0 到 1000，常用的中间件优先级如下：

- **0-400**：高优先级，通常用于处理请求和响应的核心逻辑。
- **500-600**：中等优先级，通常用于处理请求和响应的扩展功能。
- **700-1000**：低优先级，通常用于处理请求和响应的次要逻辑。

### 常见使用场景

- **身份验证**：你可以使用下载器中间件来处理身份验证，例如通过 `HttpAuthMiddleware` 或自定义中间件来添加认证信息。
- **代理切换**：你可以使用下载器中间件来动态切换代理服务器，避免 IP 被封禁。
- **请求重试**：你可以使用 `RetryMiddleware` 来自动重试失败的请求，或者编写自定义中间件来实现更复杂的重试逻辑。
- **响应缓存**：你可以编写下载器中间件来缓存响应，减少重复抓取的次数。
- **日志记录**：你可以使用蜘蛛中间件或下载器中间件来记录请求和响应的日志，帮助调试和监控爬虫的行为。
- **数据清洗**：你可以使用蜘蛛中间件来清洗蜘蛛返回的数据，例如去除重复项、格式化输出等。



### 加入一个中间件

#### 1. **使用内置的 `HttpAuthMiddleware`**

Scrapy 提供了内置的 `HttpAuthMiddleware`，它可以自动处理 HTTP 基本身份验证。你只需要在请求中传递用户名和密码即可。

##### 步骤：

1. **启用 `HttpAuthMiddleware`**： `HttpAuthMiddleware` 默认是启用的，因此你不需要在 `settings.py` 中手动添加它。如果你之前禁用了它，可以通过以下方式重新启用：

   ```python
   # settings.py
   
   DOWNLOADER_MIDDLEWARES = {
       'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,
   }
   ```

2. **在请求中传递认证信息**： 你可以在每个请求的 `meta` 参数中传递 `http_auth`，或者在 `settings.py` 中全局设置默认的用户名和密码。

   - **方法一：在每个请求中传递认证信息**：

     ```python
     import scrapy
     
     class AuthenticatedSpider(scrapy.Spider):
         name = 'authenticated_spider'
         start_urls = ['https://example.com/protected-page']
     
         def start_requests(self):
             for url in self.start_urls:
                 yield scrapy.Request(
                     url=url,
                     meta={'http_auth': {'username': 'your_username', 'password': 'your_password'}}
                 )
     ```

   - **方法二：在 `settings.py` 中全局设置默认的用户名和密码**：

     ```python
     # settings.py
     
     HTTPAUTH_USERNAME = 'your_username'
     HTTPAUTH_PASSWORD = 'your_password'
     ```

     这样，所有请求都会自动使用这些认证信息进行身份验证。

#### 2. **编写自定义的身份验证中间件**

如果你需要更复杂的认证逻辑，例如 API 密钥、OAuth 认证、或基于会话的身份验证，你可以编写自定义的下载器中间件来处理这些情况。

##### 示例 1：API 密钥认证

假设你需要在每个请求的头部添加一个 API 密钥来进行身份验证。你可以编写一个自定义的下载器中间件来实现这一点。

```python
# middlewares.py

from scrapy import signals

class ApiKeyDownloaderMiddleware:
    # This method is used by Scrapy to create your middleware.
    @classmethod
    def from_crawler(cls, crawler):
        # 获取 API 密钥的配置
        api_key = crawler.settings.get('API_KEY')
        if not api_key:
            raise ValueError("API_KEY must be set in settings.")
        
        middleware = cls(api_key)
        crawler.signals.connect(middleware.spider_opened, signal=signals.spider_opened)
        return middleware

    def __init__(self, api_key):
        self.api_key = api_key

    def process_request(self, request, spider):
        # 在每个请求的头部添加 API 密钥
        request.headers['Authorization'] = f'Bearer {self.api_key}'
        return None

    def spider_opened(self, spider):
        spider.logger.info(f'Spider opened: {spider.name} with API Key: {self.api_key}')
```

##### 启用自定义中间件：

```python
# settings.py

DOWNLOADER_MIDDLEWARES = {
    'mySpider.middlewares.ApiKeyDownloaderMiddleware': 543,
}

# 设置 API 密钥
API_KEY = 'your_api_key_here'
```

##### 示例 2：基于会话的身份验证

如果你需要通过登录页面获取会话 cookie，并在后续请求中使用该会话进行身份验证，你可以编写一个自定义的下载器中间件来管理会话。

```python
# middlewares.py

import scrapy
from scrapy.http import Request, FormRequest
from scrapy.exceptions import IgnoreRequest

class SessionDownloaderMiddleware:
    @classmethod
    def from_crawler(cls, crawler):
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def spider_opened(self, spider):
        # 登录并获取会话 cookie
        login_url = 'https://example.com/login'
        username = 'your_username'
        password = 'your_password'

        # 发送登录请求
        return scrapy.FormRequest(
            url=login_url,
            formdata={'username': username, 'password': password},
            callback=self.after_login
        )

    def after_login(self, response):
        # 检查登录是否成功
        if "Login failed" in response.body.decode('utf-8'):
            self.logger.error("Login failed")
            raise IgnoreRequest("Login failed")

        # 登录成功后继续爬取其他页面
        self.logger.info("Login successful")
        return None

    def process_request(self, request, spider):
        # 确保每个请求都带有会话 cookie
        if not request.cookies:
            request.cookies = self.session_cookies
        return None

    def process_response(self, request, response, spider):
        # 更新会话 cookie（如果需要）
        if response.status == 200 and 'Set-Cookie' in response.headers:
            self.session_cookies = response.headers.getlist('Set-Cookie')
        return response
```

##### 启用自定义中间件：

```python
# settings.py

DOWNLOADER_MIDDLEWARES = {
    'mySpider.middlewares.SessionDownloaderMiddleware': 543,
}
```

#### 3. **使用 OAuth 2.0 身份验证**

如果你需要使用 OAuth 2.0 进行身份验证，通常你需要先获取访问令牌（Access Token），然后在每个请求的头部添加该令牌。你可以编写一个自定义的下载器中间件来处理 OAuth 2.0 的认证流程。

##### 示例 3：OAuth 2.0 认证

```python
# middlewares.py

import requests
from scrapy import signals

class OAuthDownloaderMiddleware:
    @classmethod
    def from_crawler(cls, crawler):
        oauth_client_id = crawler.settings.get('OAUTH_CLIENT_ID')
        oauth_client_secret = crawler.settings.get('OAUTH_CLIENT_SECRET')
        oauth_token_url = crawler.settings.get('OAUTH_TOKEN_URL')

        if not all([oauth_client_id, oauth_client_secret, oauth_token_url]):
            raise ValueError("OAUTH_CLIENT_ID, OAUTH_CLIENT_SECRET, and OAUTH_TOKEN_URL must be set in settings.")

        middleware = cls(oauth_client_id, oauth_client_secret, oauth_token_url)
        crawler.signals.connect(middleware.spider_opened, signal=signals.spider_opened)
        return middleware

    def __init__(self, client_id, client_secret, token_url):
        self.client_id = client_id
        self.client_secret = client_secret
        self.token_url = token_url
        self.access_token = None

    def get_access_token(self):
        # 获取 OAuth 2.0 访问令牌
        data = {
            'grant_type': 'client_credentials',
            'client_id': self.client_id,
            'client_secret': self.client_secret
        }
        response = requests.post(self.token_url, data=data)
        if response.status_code == 200:
            self.access_token = response.json().get('access_token')
        else:
            raise Exception("Failed to obtain access token")

    def process_request(self, request, spider):
        # 如果没有访问令牌，先获取
        if not self.access_token:
            self.get_access_token()

        # 在每个请求的头部添加访问令牌
        request.headers['Authorization'] = f'Bearer {self.access_token}'
        return None

    def spider_opened(self, spider):
        spider.logger.info(f'Spider opened: {spider.name} with OAuth 2.0 authentication')
```

##### 启用自定义中间件：

```python
# settings.py

DOWNLOADER_MIDDLEWARES = {
    'mySpider.middlewares.OAuthDownloaderMiddleware': 543,
}

# 设置 OAuth 2.0 相关配置
OAUTH_CLIENT_ID = 'your_client_id'
OAUTH_CLIENT_SECRET = 'your_client_secret'
OAUTH_TOKEN_URL = 'https://example.com/oauth/token'
```

##### 总结

通过 Scrapy 的中间件机制，你可以轻松地实现各种身份验证方式。无论是简单的 HTTP 基本身份验证，还是复杂的 OAuth 2.0 或基于会话的身份验证，都可以通过编写自定义中间件来实现。以下是一些常见的身份验证场景及其对应的实现方式：

- **HTTP 基本身份验证**：使用内置的 `HttpAuthMiddleware` 或在请求中传递认证信息。
- **API 密钥认证**：编写自定义中间件，在每个请求的头部添加 API 密钥。
- **基于会话的身份验证**：编写自定义中间件，管理登录会话并确保每个请求都带有会话 cookie。
- **OAuth 2.0 认证**：编写自定义中间件，获取并管理访问令牌。

## 六、scrapy-redis组件学习

### 简介

Scrapy_redis ： Redis-based components for Scrapy.

Github地址：https://github.com/rmax/scrapy-redis

在这个地址中存在三个demo，后续我们对scrapy_redis的学习会通过这三个demo展开

安装方式:pip install scrapy-redis

### 组件作用

Scrapy_redis在scrapy的基础上实现了更多，更强大的功能，具体体现在：

- 请求对象的持久化
- 去重的持久化
- 和实现分布式

![image-20241212153910996](.\image\image-20241212153910996.png)

加入scrapy-redis可以实现如上结构。

#### redis配置

默认情况下，Redis服务值运行本地电脑访问，即Redis运行在哪台电脑就只允许这台电脑上的软件连接试用它

既然我们要做分布式爬取，就意味着别的电脑也需要访问Redis，因此需要对Redis进行配置

配置参考地址：https://www.cnblogs.com/masonblog/p/12726914.html

1. 别忘了修改爬虫的配置文件`settings.py`，将Redis服务器地址改为你运行Redis的服务器的ip
2. 服务器安装：https://blog.csdn.net/m0_60028455/article/details/125316625

#### redis服务端和客户端的启动

- `/etc/init.d/redis-server start` 启动服务端
- `redis-cli -h -p <端口号>` 客户端启动

#### redis中的常见命令

- 命令文档:[http://doc.redisfans.com](http://doc.redisfans.com/)
- 可视化工具:https://gitee.com/qishibo/AnotherRedisDesktopManager/releases

1. `select 1` 切换db
2. `keys *` 查看所有的键
3. `type 键` 查看键的类型
4. `flushdb` 清空db
5. `flushall` 清空数据库
6. `lrange 键 0 -1` 查看列表数据

#### redis命令的复习

redis的命令很多，这里我们简单复习后续会使用的命令

```bash
### 字符串（String）
SET mykey "Hello, Redis!"	#设置指定键的值。
GET mykey	#获取指定键的值。
INCR mycounter	#将键的值递增 1（仅适用于整数）。
DECR mycounter	#将键的值递减 1（仅适用于整数）。
INCRBY mycounter 5	#将键的值递增指定的增量。
DECRBY mycounter 3	#将键的值递减指定的减量。
APPEND mykey " World"	#将值追加到指定键的现有值后面。
STRLEN mykey	#返回指定键的值的长度。
GETRANGE mykey 0 5	#获取指定键的值的子字符串。
SETRANGE mykey 6 "Redis"	#从指定偏移量开始修改键的值。
GETSET mykey "New Value"	#设置键的值，并返回旧值。
MSET key1 "value1" key2 "value2"	#同时设置多个键的值。
MGET key1 key2	#获取多个键的值。

### 列表（List）
LPUSH mylist "one" "two" "three"	#向 mylist 从左边添加三个值，可以任意数量
RPUSH mylist "four" "five"	#将一个或多个值插入到列表的右边。
LPOP mylist	#从列表的右边弹出并返回最后一个元素。
RPOP mylist	#从列表的右边弹出并返回最后一个元素。
LRANGE mylist 0 -1	#返回mylist 中所有的值
LLEN mylist	#返回 mylist 的长度
LINDEX mylist 2	#返回列表中指定索引的元素。
LSET mylist 1 "new_value"	#设置列表中指定索引的元素。
LTRIM mylist 0 2	#修剪列表，保留指定范围内的元素。
BLPOP mylist 0	#阻塞式 LPOP，等待直到有元素可用或超时。
BRPOP mylist 0	#阻塞式 RPOP，等待直到有元素可用或超时。

### 集合（Set）
SADD myset "apple" "banana" "orange"	# 向 myset 中添加数据
SMEMBERS myset	# 获取 myset 中所有的元素
SCARD myset	# 获取 myset 中元素的数量
SISMEMBER myset "apple"	#检查集合中是否存在指定成员。
SREM myset "banana"	#从集合中移除一个或多个成员。
SPOP myset 2	#随机移除并返回一个或多个成员。
SRANDMEMBER myset 2	#随机返回一个或多个成员，但不移除。
SINTER set1 set2	#返回多个集合的交集。
SUNION set1 set2	#返回多个集合的并集。
SDIFF set1 set2	#返回多个集合的差集。
SMOVE myset1 myset2 "apple"	#将成员从一个集合移动到另一个集合。

### 有序集合（Sorted Set）
ZADD myzset 1 "one" 2 "two" 3 "three"	# 向 myzset 中添加一个值和分数
ZRANGE myzset 0 -1 WITHSCORES	#返回有序集合中指定范围的成员，按分数升序排列。
ZCARD myzset	#返回有序集合中成员的数量。
ZSCORE myzset "two"	#返回有序集合中指定成员的分数。
ZRANK myzset "two"	#返回有序集合中指定成员的排名（从 0 开始）。
ZREVRANK myzset "two"	#返回有序集合中指定成员的逆向排名（从 0 开始）。
ZRANGE myzset 0 -1 WITHSCORES	#返回有序集合中指定范围的成员，按分数升序排列。
ZREVRANGE myzset 0 -1 WITHSCORES	#返回有序集合中指定范围的成员，按分数降序排列。
ZCOUNT myzset 1 2	#返回有序集合中分数在指定范围内的成员数量。
ZREM myzset "two"	#从有序集合中移除一个或多个成员。
ZINCRBY myzset 1 "two"	#将有序集合中指定成员的分数增加指定的增量。
ZLEXCOUNT myzset - +	#返回有序集合中按字典顺序在指定范围内的成员数量。
ZRANGEBYLEX myzset - +	#返回有序集合中按字典顺序在指定范围内的成员。
ZREMRANGEBYRANK myzset 0 1	#移除有序集合中排名在指定范围内的成员。
ZREMRANGEBYSCORE myzset 1 2	#移除有序集合中分数在指定范围内的成员。
ZREMRANGEBYLEX myzset - [c	#移除有序集合中按字典顺序在指定范围内的成员。

### 哈希（Hash）
HSET user:1000 name "Alice" age 30	#设置哈希表中指定字段的值。
HGET user:1000 name	#获取哈希表中指定字段的值。
HMSET user:1000 name "Alice" age 30	#同时设置多个字段的值（已废弃，建议使用 HSET）。
HMGET user:1000 name age	#获取多个字段的值。
HGETALL user:1000	#返回哈希表中所有的字段和值。
HDEL user:1000 age	#删除一个或多个字段。
HEXISTS user:1000 name	#检查哈希表中是否存在指定字段。
HLEN user:1000	#返回哈希表中字段的数量。
HINCRBY user:1000 age 1	#返回哈希表中字段的数量。
HINCRBYFLOAT user:1000 score 0.5	#将哈希表中指定字段的值递增指定的浮点增量。
HKEYS user:1000	#返回哈希表中所有的字段。
HVALS user:1000	#返回哈希表中所有的值。

```

#### scrapy-redis的核心组件

##### 1. Scheduler（调度）：

`scrapy`改造了`python`本来的`collection.deque`(双向队列)形成了自己`scrapy queue`，而`scrapy-redis` 的解决是把这个`scrapy queue`换成`redis`数据库，从同一个`redis-server`存放要爬取的`request`，便能让多个`spider`去同一个数据库里读取。

Scrapy中跟“待爬队列”直接相关的就是调度器`Scheduler`，它负责对新的request进行入列操作（加入Scrapy queue），取出下一个要爬取的request（从Scrapy queue中取出）等操作。它把待爬队列按照优先级建立了一个字典结构，比如：

```
{
    优先级0 : 队列0
    优先级1 : 队列1
    优先级2 : 队列2
}
```

然后根据request中的优先级，来决定该入哪个队列，出列时则按优先级较小的优先出列。为了管理这个比较高级的队列字典，Scheduler需要提供一系列的方法。但是原来的Scheduler已经无法使用，所以使用Scrapy-redis的scheduler组件。

##### 2. Duplication Filter（去重）：

Scrapy中用集合实现这个request去重功能，Scrapy中把已经发送的request指纹放入到一个集合中，把下一个request的指纹拿到集合中比对，如果该指纹存在于集合中，说明这个request发送过了，如果没有则继续操作。这个核心的判重功能是这样实现的：

```python
def request_seen(self, request):
    # self.request_figerprints就是一个指纹集合  
    fp = self.request_fingerprint(request)

    # 这就是判重的核心操作  
    if fp in self.fingerprints:
        return True
    self.fingerprints.add(fp)
    if self.file:
        self.file.write(fp + os.linesep)
```

在scrapy-redis中去重是由`Duplication Filter`组件来实现的，它通过redis的set 不重复的特性，巧妙的实现了Duplication Filter去重。scrapy-redis调度器从引擎接受request，将request的指纹存⼊redis的set检查是否重复，并将不重复的request push写⼊redis的 request queue。

引擎请求request(Spider发出的）时，调度器从redis的request queue队列⾥里根据优先级pop 出⼀个request 返回给引擎，引擎将此request发给spider处理。

##### 3. Item Pipline（管道）：

引擎将(Spider返回的)爬取到的Item给`Item Pipeline`，`scrapy-redis` 的`Item Pipeline`将爬取到的 `Item `存⼊`redis`的` items queue`。

修改过`Item Pipeline`可以很方便的根据`key`从`items queue`提取`item`，从而实现 `items processes`集群。

##### 4. Base Spider（爬虫）：

不再使用`scrapy`原有的`Spider`类，重写的`RedisSpider`继承了`Spider`和`RedisMixin`这两个类，`RedisMixin`是用来从`redis`读取`url`的类。

当我们生成一个Spider继承RedisSpider时，调用setup_redis函数，这个函数会去连接redis数据库，然后会设置signals(信号)：

- 一个是当spider空闲时候的signal，会调用spider_idle函数，这个函数调用`schedule_next_request`函数，保证spider是一直活着的状态，并且抛出DontCloseSpider异常。
- 一个是当抓到一个item时的signal，会调用item_scraped函数，这个函数会调用`schedule_next_request`函数，获取下一个request。



#### scrapy-redis的核心配置

首先最主要的是，需要将调度器的类和去重的类替换为 `Scrapy-Redis `提供的类，在 `settings.py` 里面添加如下配置即可：

```python
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
# 布隆过滤器
# pip install scrapy-redis-bloomfilter
DUPEFILTER_CLASS = "scrapy_redis_bloomfilter.dupefilter.RFPDupeFilter"
```

直接在 `settings.py` 里面配置为 `REDIS_URL` 变量即可：

```python
redis://[:password]@host:port/db

REDIS_URL = 'redis://172.25.197.89:6379/0'
```

##### 1.配置调度队列

此项配置是可选的，默认使用 `PriorityQueue`。如果想要更改配置，可以配置 SCHEDULER_QUEUE_CLASS 变量，如下所示：

```python
SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'  # 使用有序集合来存储
SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue'  # 先进先出
SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue'  # 先进后出、后进先出
```

##### 2.配置redis-item

此配置可自行选择,想存放在redis可以放开

```python
ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline' : 400,
}
```

##### 3.爬虫文件配置

修改爬虫 的继承类,给上对应的redis键名

```python
from scrapy_redis.spiders import RedisSpider
class DoubanSpider(RedisSpider):
    # 爬虫的名字
    name = 'douban'
    # redis的键名
    redis_key = 'douban:start'
```

##### 4.配置持久化

此配置是可选的，默认是 False。Scrapy-Redis 默认会在爬取全部完成后清空爬取队列和去重指纹集合。初始第一个网址一定会进行请求,后面的重复方式不会进行请求。

如果不想自动清空爬取队列和去重指纹集合，可以增加如下配置：

```python
SCHEDULER_PERSIST = True   # 底层 就是监听爬虫信号  触发redis指令 清空数据库
# SCHEDULER_PERSIST 控制爬虫是否在关闭时保持状态,True 不清空数据
```

##### 5.配置重爬

此配置是可选的，默认是 False。如果配置了持久化或者强制中断了爬虫，那么爬取队列和指纹集合不会被清空，爬虫重新启动之后就会接着上次爬取。如果想重新爬取，我们可以配置重爬的选项

```python
SCHEDULER_FLUSH_ON_START = True
# SCHEDULER_FLUSH_ON_START 控制是否在启动时清空请求队列,设置为 True 之后，爬虫每次启动时，爬取队列和指纹集合都会清空。
```

这样将 SCHEDULER_FLUSH_ON_START 设置为 True 之后，爬虫每次启动时，爬取队列和指纹集合都会清空。所以要做分布式爬取，我们必须保证只能清空一次，否则每个爬虫任务在启动时都清空一次，就会把之前的爬取队列清空，势必会影响分布式爬取。

如果你需要在每次启动时都重新开始爬取，可以将两者都设置为 `False`。如果需要断点续爬，可以将 `SCHEDULER_FLUSH_ON_START` 设置为 `False`，并将 `SCHEDULER_PERSIST` 设置为 `True`。



## 七、项目实战

#### 7.1 项目分析

**目标网址：http://search.dangdang.com/?key=Python&act=input**

**采集需求：** 采集目标标题，价格，作者，上线时间， 介绍详情，出版社

#### 7.2 创建项目

```python
# 创建项目
scrapy startproject dd_project
# 创建爬虫  
scrapy genspider dd_spider search.dangdang.com
```

#### 7.3 爬虫文件

```python
import scrapy
from scrapy_redis.spiders import RedisSpider
from DD.items import DdItem


class DangdangSpider(RedisSpider):
    # 爬虫名称
    name = "dangdang"
    # allowed_domains = ["search.dangdang.com"]
    # start_urls = ["http://search.dangdang.com/"]
    # redis键名称
    redis_key = 'dd:start'

    def parse(self, response, **kwargs):
        li_list = response.xpath('//ul[@class="bigimg"]/li')
        for li in li_list:
            item = DdItem()
            item['title'] = li.xpath('./a/@title').extract_first()
            item['price'] = li.xpath('./p[@class="price"]/span[1]/text()').extract_first()
            item['author'] = li.xpath('./p[@class="search_book_author"]/span[1]/a[1]/@title').extract_first()
            item['date_data'] = li.xpath('./p[@class="search_book_author"]/span[last()-1]/text()').extract_first()
            item['detail'] = li.xpath('./p[@class="detail"]/text()').extract_first() if li.xpath(
                './p[@class="detail"]/text()') else '空'
            item['producer'] = li.xpath(
                './p[@class="search_book_author"]/span[last()]/a/text()').extract_first() if li.xpath(
                './p[@class="search_book_author"]/span[last()]/a/text()') else '空'
            print(item)
            yield item

        if response.xpath('//ul[@name="Fy"]/li[@class="next"]/a/@href').extract_first() != None:
            next_url = response.urljoin(response.xpath('//ul[@name="Fy"]/li[@class="next"]/a/@href').extract_first())
            yield scrapy.Request(url=next_url, callback=self.parse)


if __name__ == '__main__':
    from scrapy import cmdline

    cmdline.execute('scrapy crawl dangdang'.split())
```

#### 7.4 item文件

```python
import scrapy


class DdItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    title = scrapy.Field()
    price = scrapy.Field()
    author = scrapy.Field()
    date_data = scrapy.Field()
    detail = scrapy.Field()
    producer = scrapy.Field()
```

#### 7.5 管道文件

```python
import pymysql

class DdMySQLPipeline:
    def open_spider(self, spider):
        # 判断是哪个爬虫  名字不同可能执行的爬虫项目也不同
        if spider.name == 'dangdang':
            self.db = pymysql.connect(host="localhost", user="root", password="root", db="spiders")
            self.cursor = self.db.cursor()
            # 创建变语法
            sql = '''
                                CREATE TABLE IF NOT EXISTS dangdang(
                                    id int primary key auto_increment not null,
                                    title VARCHAR(255) NOT NULL, 
                                    price VARCHAR(255) NOT NULL, 
                                    author VARCHAR(255) NOT NULL, 
                                    date_data VARCHAR(255) NOT NULL, 
                                    detail TEXT, 
                                    producer VARCHAR(255) NOT NULL)
                                '''
            try:
                self.cursor.execute(sql)
                print("CREATE TABLE SUCCESS.")
            except Exception as ex:
                print(f"CREATE TABLE FAILED,CASE:{ex}")

    def process_item(self, item, spider):
        # SQL 插入语句
        sql = 'INSERT INTO dangdang(id, title, price, author, date_data, detail, producer) values(%s, %s, %s, %s, %s, %s, %s)'
        # 执行 SQL 语句
        try:
            self.cursor.execute(sql, (0, item['title'], item['price'], item['author'], item['date_data'], item['detail'], item['producer']))
            # 提交到数据库执行
            self.db.commit() 
            print('mysql数据插入成功...')
        except Exception as e:
            print(f'数据插入失败: {e}')
            # 如果发生错误就回滚
            self.db.rollback()
        # 不return的情况下，另一个权重较低的pipeline将不会获得item,否则后一个pipeline取到的数据为None值
        return item

    def close_spider(self, spider):  # 在爬虫关闭的时候仅执行一次
        # 关闭文件
        if spider.name == 'dangdang':
            self.db.close()
```

#### 7.6 设置文件

```python
# 持久化配置
SCHEDULER_PERSIST = True
# 使用scrapy-redis调度器
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
# scrapy-redis指纹过滤器,去重
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
# redis链接地址
REDIS_URL = 'redis://:密码@ip:端口号/0'
# redis://:<password>@<host>:<port>/<db_index>
# 任务的优先级别
SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'
# 存放的管道
ITEM_PIPELINES = {
    "scrapy_redis.pipelines.RedisPipeline": 300,
    'dd_project.pipelines.DdMySQLPipeline': 301,
}

DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'
}
```

#### 7.7 分配任务

用来往redis插入url，生产者身份。---单独启动就行，正式项目应该是爬取每个需要的页地址写入redis

```python
import redis
import json
res = redis.Redis()
res.lpush('dd:start', 'http://search.dangdang.com/?key=python&act=input&page_index=1')
# res.lpush('dd:start', 'http://search.dangdang.com/?key=java&act=input&page_index=1')
# res.lpush('dd:start', 'http://search.dangdang.com/?key=go&act=input&page_index=1')
# for i in res.lrange('dangdang:items', 0, -1):
#     print(json.loads(i))
```

### scrapy-redis发送post请求

- 在当前案例中我们需要掌握如何使用`scrapy-redis`发送`post`请求并完成翻页。
- 站点请求地址：http://www.cninfo.com.cn/new/commonUrl?url=disclosure/list/notice#szse

```python
import json
from scrapy import cmdline
from scrapy.http import FormRequest
from scrapy_redis.spiders import RedisSpider


class JcSpider(RedisSpider):
    name = "jc"
    allowed_domains = ["www.cninfo.com.cn"]
    # start_urls = ["http://www.cninfo.com.cn/"]
    redis_key = 'jc:start_urls'

    def make_request_from_data(self, data):
        url = 'http://www.cninfo.com.cn/new/disclosure'
        """
        :param data: 是scrapy-redis读取redis中的[url, form_data, meta]
        :return:
        """
        data = json.loads(data)
        form_data = data.get('form_data')
        print('12323', form_data)

        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }

        return FormRequest(url=url, headers=headers, formdata=form_data, callback=self.parse)

    def parse(self, response, **kwargs):
        print('返回信息:', response.json())


if __name__ == '__main__':
    cmdline.execute('scrapy crawl jc'.split())
```

- redis提交数据代码

```python
import json
import redis


def push_start_url_data(db, request_obj):
    """
    :param db: redis链接对象
    :param request_obj: {'url':url, 'form_data':form_data, 'meta':meta}
    :return:
    """
    db.lpush('jc:start_urls', request_obj)


if __name__ == '__main__':
    redis_client = redis.Redis()
    # 需要将表单中的数字类型强转为字符串类型
    for page in range(1, 26):
        form_data = {
            'column': 'szse_latest',
            'pageNum': str(page),
            'pageSize': '30',
            'sortName': '',
            'sortType': '',
            'clusterFlag': 'true'
        }

        request_data = {
            'form_data': form_data
        }

        push_start_url_data(redis_client, json.dumps(request_data))
    redis_client.close()
```

- 配置文件

```python
"""scrapy-redis配置"""
# 调度器类
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
# 指纹去重类
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
# 是否在关闭时候保留原来的调度器和去重记录，True=保留，False=清空
SCHEDULER_PERSIST = True
# Redis服务器地址
REDIS_URL = "redis://127.0.0.1:6379/0"
```

#### 其他post方式

为了使 `scrapy-redis` 能够发送 POST 请求，最直接的方法之一是重写 `start_requests` 方法。在这个方法中，我们可以使用 `scrapy.FormRequest` 来构造 POST 请求，并指定请求的 URL 和参数。例如：

```python
import scrapy
from scrapy_redis.spiders import RedisSpider

class MySpider(RedisSpider):
    name = 'my_spider'
    allowed_domains = ['example.com']

    def start_requests(self):
        # 从Redis队列中获取URL
        for url in self.start_urls:
            yield scrapy.FormRequest(
                url=url,
                formdata={'key': 'value'},  # 这里填写POST请求所需的表单数据
                callback=self.parse
            )
```

在某些情况下，您可能希望基于页面上的表单来构建 POST 请求。这时可以使用 `FormRequest.from_response` 方法，它会自动根据响应中的表单元素生成正确的 POST 请求。这通常用于模拟登录或提交表单操作。例如：

```python
def parse(self, response):
    # 假设我们正在解析一个包含登录表单的页面
    return scrapy.FormRequest.from_response(
        response,
        formdata={'username': 'user', 'password': 'pass'},
        callback=self.after_login
    )

def after_login(self, response):
    # 登录成功后的处理逻辑
    pass
```

# FEAPDER

## 简介

***\*feapder\**** 是一款上手简单，功能强大的Python爬虫框架，使用方式类似`scrapy`，方便由`scrapy`框架切换过来，框架内置3种爬虫：

### 常用场景

- `AirSpider`爬虫比较轻量，学习成本低。面对一些数据量较少，无需断点续爬，无需分布式采集的需求，可采用此爬虫。

- `Spider`是一款基于`redis`的分布式爬虫，适用于海量数据采集，支持断点续爬、爬虫报警、数据自动入库等功能

- `BatchSpider`是一款分布式批次爬虫，对于需要周期性采集的数据，优先考虑使用本爬虫。

### 文档、下载、框架结构、运行流程

国内文档：[https://boris-code.gitee.io/feapder](https://boris-code.gitee.io/feapder)

- Python 3.6.0+
- Works on Linux, Windows, macOS

```cmd
pip3 install feapder -i https://repo.huaweicloud.com/repository/pypi/simple/
```

![image-20241216180644151](.\image\image-20241216180644151.png)

- spider **框架调度核心**
- parser_control **模版控制器**，负责调度parser
- collector **任务收集器**，负责从任务队里中批量取任务到内存，以减少爬虫对任务队列数据库的访问频率及并发量
- parser **数据解析器**
- start_request 初始任务下发函数
- item_buffer **数据缓冲队列**，批量将数据存储到数据库中
- request_buffer **请求任务缓冲队列**，批量将请求任务存储到任务队列中
- request **数据下载器**，封装了requests，用于从互联网上下载数据
- response **请求响应**，封装了response, 支持xpath、css、re等解析方式，自动处理中文乱码

**运行流程**

1.spider调度**start_request**生产任务
2.**start_request**下发任务到request_buffer中
3.spider调度**request_buffer**批量将任务存储到任务队列数据库中
4.spider调度**collector**从任务队列中批量获取任务到内存队列
5.spider调度**parser_control**从collector的内存队列中获取任务
6.**parser_control**调度**request**请求数据
7.**request**请求与下载数据
8.request将下载后的数据给**response**，进一步封装
9.将封装好的**response**返回给**parser_control**（图示为多个parser_control，表示多线程）
10.parser_control调度对应的**parser**，解析返回的response（图示多组parser表示不同的网站解析器）
11.parser_control将parser解析到的数据item及新产生的request分发到**item_buffer**与**request_buffer**
12.spider调度**item_buffer**与**request_buffer**将数据批量入库



## 使用方式

### 1、AirSpider使用

#### 1.1 创建项目

```python
feapder create -s douban 
```

- AirSpider：轻量级爬虫

  简单易学，适合新人入门、少量数据爬取。爬虫一旦停止，所有未完成的任务会被抛弃。

- Spider：分布式爬虫

  **分布式架构**：基于redis实现分布式架构，可以有效管理大规模的数据爬取任务并行处理多个请求。

  **断点续爬**：爬虫中断后，可以保存当前状态并下次启动时继续未完成的工作，确保数据完整性。

  **自动数据入库**：支持将抓取的数据存入数据库中，简化后续的数据处理流程。

  **强大的扩展性**：可以通过添加节点来增强系统的吞吐量，适应不断增长的数据量和复杂度。

- TaskSpider：任务型爬虫

  **任务驱动**：支持对接任务表，如myslq、redis等，允许开发者定义具体的抓取任务并将其分配给不同的爬虫实例执行

  **灵活调度**：可以根据实际需求动态调整任务优先级，合理安排资源分配，提高整体效率。

  **易于维护**：通过集中管理任务队列，便于监控和管理各个爬虫的状态，及时发现并解决问题。

- BatchSpider：批次爬虫

  **周期性采集**：专门为定期重复抓取同一类数据而设计的，它可以记录每次抓取的状态，确保每次都能获取最新的信息。

  **全量抓取**：为了保证数据的准确性，在每次允许时重新抓取整个数据集，而不是仅仅增量更新

  **监控预警**：提供完善的监控机制，能够在出现问题时及时发出警报，帮助管理员快速响应。



#### 1.2 代码详解

默认生成的代码继承了feapder.AirSpider，包含 `start_requests` 及 `parser` 两个函数，含义如下：

1. feapder.AirSpider：轻量爬虫基类
2. start_requests：初始任务下发入口
3. feapder.Request：基于requests库类似，表示一个请求，支持requests所有参数，
4. parser：数据解析函数
5. response：请求响应的返回体，支持xpath、re、css等解析方式，详情可参考[Response](https://boris-code.gitee.io/feapder/#/source_code/Response)

除了start_requests、parser两个函数,系统还内置了下载中间件等函数。



#### 1.3 使用案例

```python
import feapder


class Douban(feapder.AirSpider):
    __custom_setting__ = dict(
        REQUEST_FILTER_ENABLE=True,  # request 去重
        REQUEST_FILTER_SETTING=dict(
            filter_type=4,  # 永久去重（BloomFilter） = 1 、内存去重（MemoryFilter） = 2、 临时去重（ExpireFilter）= 3、 轻量去重（LiteFilter）= 4
        ),
    )

    def start_requests(self):
        for i in range(11):
            yield feapder.Request(f"https://movie.douban.com/top250?start={i*25}&filter=")

    def parse(self, request, response):
        li_list = response.xpath('//ol/li/div[@class="item"]')
        # print(response.text)

        for li in li_list:
            item = {}
            item['title'] = li.xpath('./div[@class="info"]/div/a/span[1]/text()').extract_first()
            item['detail_url'] = li.xpath('./div[@class="info"]/div/a/@href').extract_first()
            item['score'] = li.xpath('.//div[@class="star"]/span[2]/text()').extract_first()
            yield feapder.Request(item['detail_url'], callback=self.parser_detail, item=item)

    def parser_detail(self, request, response):
        item = request.item
        if response.xpath('//div[@class="indent"]/span[@class="all hidden"]//text()'):
            item['detail_text'] = response.xpath('//div[@class="indent"]/span[@class="all hidden"]//text()').extract_first().strip()
        else:
            item['detail_text'] = response.xpath('//div[@class="indent"]/span[1]//text()').extract_first().strip()
        print(item)

if __name__ == "__main__":
    Douban(thread_count=5).start()
```



#### 1.4 数据入库

```
feapder`给我们提供了方法`MysqlDB
```

- 参考：https://feapder.com/#/source_code/MysqlDB

```python
from feapder.db.mysqldb import MysqlDB

db = MysqlDB(
    ip="localhost", port=3306, db="spiders", user_name="root", user_pass="root"
)

sql = '''
CREATE TABLE IF NOT EXISTS douban(
    id int primary key auto_increment not null,
    title VARCHAR(255) NOT NULL, 
    score VARCHAR(255) NOT NULL, 
    detail_text TEXT)
'''
db.execute(sql)
sql1 = "insert ignore into douban (id,title, score, detail_text) values ('%s', '%s','%s', '%s')" % (
0, '爱情动作片', '10', '爱情动作商业片')
db.add(sql1)
```



#### 1.5 创建配置文件

```
feapder create --setting
```

##### 1.5.1 激活配置

```python
# MYSQL
MYSQL_IP = "localhost"
MYSQL_PORT = 3306
MYSQL_DB = "spiders"
MYSQL_USER_NAME = "root"
MYSQL_USER_PASS = "root"
```

##### 1.5.2 创建item文件 

```python
feapder create -i douban     # douban=表名
from feapder import Item


class DoubanItem(Item):
    """
    This class was generated by feapder
    command: feapder create -i douban
    """

    __table_name__ = "douban"

    def __init__(self, *args, **kwargs):
        # self.id = None
        self.title = None
        self.score = None
        self.detail_text = None
```



##### 1.5.3 项目引入

```python
from douban_item import DoubanItem
def parse(self, request, response):
    li_list = response.xpath('//ol/li/div[@class="item"]')
    # print(response.text)

    for li in li_list:
        item = DoubanItem()
        item['title'] = li.xpath('./div[@class="info"]/div/a/span[1]/text()').extract_first()

        item['score'] = li.xpath('.//div[@class="star"]/span[2]/text()').extract_first()
        detail_url = li.xpath('./div[@class="info"]/div/a/@href').extract_first()
        yield feapder.Request(detail_url, callback=self.parser_detail, item1=item)

def parser_detail(self, request, response):
    item = request.item1
    if response.xpath('//div[@class="indent"]/span[@class="all hidden"]//text()'):
        item['detail_text'] = response.xpath(
            '//div[@class="indent"]/span[@class="all hidden"]//text()').extract_first().strip()
    else:
        item['detail_text'] = response.xpath('//div[@class="indent"]/span[1]//text()').extract_first().strip()
    yield item
```

#### 1.6下载器中间件

- 下载中间件用于在请求之前，对请求做一些处理，如添加cookie、header等
- 默认所有的解析函数在请求之前都会经过此下载中间件

```Python
def start_requests(self):
    for i in range(1):
        yield feapder.Request(f"https://movie.douban.com/top250?start={i * 25}&filter=", download_midware=self.download_midware)

def download_midware(self, request):
    request.headers = {
        'aa': 'baichuan'
    }
    request.proxies = {
        "http": "http://xxx.xxx.xxx.xxx:xxxx"
    }
    return request

def parse(self, request, response):
    print(response.headers)
```

#### 1.7自定义下载中间件

- 与自定义解析函数类似，下载中间件也支持自定义，只需要在feapder.Request参数里指定个`download_midware`回调即可，写法如下：
- 自定义的下载中间件只有指定的请求才会经过。其他未指定下载中间件的请求，还是会经过默认的下载中间件

```python
    def start_requests(self):
        # for i in range(11):
            yield feapder.Request(f"https://movie.douban.com/top250?start={1 * 25}&filter=", download_midware=self.custom_download_midware)

    def custom_download_midware(self, request):
        request.headers = {'bb': '11111'}
        return request
```

#### 1.8 校验

- feapder框架给到一个方法`validate`用来检验返回的数据是否正常
- 框架支持重试机制，下载失败或解析函数抛出异常会自动重试 请求。
- 默认最大重试次数为100次，我们可以引入配置文件或自定义配置来修改重试次数

```python
import feapder


class DoubanDemo(feapder.AirSpider):
    def start_requests(self):
        # for i in range(11):
        yield feapder.Request(f"https://movie.douban.com/top250?start={1 * 25}&filter=")



    def parse(self, request, response):
        pass

    def validate(self, request, response):
        print(response.status_code)
        if response.status_code != 200:
            raise Exception("response code not 200")


if __name__ == "__main__":
    DoubanDemo().start()
```



### 2、浏览器渲染下载

- 采集动态页面时（Ajax渲染的页面），常用的有两种方案。一种是找接口拼参数，这种方式比较复杂但效率高，需要一定的爬虫功底；另外一种是采用浏览器渲染的方式，直接获取源码，简单方便
- 内置浏览器渲染支持 **CHROME** 、**PHANTOMJS**、**FIREFOX**
- 在返回的Request中传递`render=True`即可

```python
def start_requests(self):
    yield feapder.Request("https://news.qq.com/", render=True)
```

#### 2.1 配置自动化

- 在setting里面改配置
- 在项目里面重写配置

```cmd
# 自动化需要加载的三个模块，如没有自行安装
pip install playwright -i https://mirrors.aliyun.com/pypi/simple/
pip install selenium
pip install webdriver_manager
```

```python
__custom_setting__ = dict(
    pool_size=1,  # 浏览器的数量
    load_images=True,  # 是否加载图片
    user_agent=None,  # 字符串 或 无参函数，返回值为user_agent
    proxy=None,  # xxx.xxx.xxx.xxx:xxxx 或 无参函数，返回值为代理地址
    headless=False,  # 是否为无头浏览器
    driver_type="CHROME",  # CHROME、PHANTOMJS、FIREFOX
    timeout=30,  # 请求超时时间
    window_size=(1024, 800),  # 窗口大小
    # 不给指定路径会出现编码问题 
    executable_path='D:\software\miniconda3\chromedriver.exe',  # 浏览器路径，默认为默认路径
    render_time=0,  # 渲染时长，即打开网页等待指定时间后再获取源码
    custom_argument=[
        "--ignore-certificate-errors",
        "--disable-blink-features=AutomationControlled",
    ],  # 自定义浏览器渲染参数
    xhr_url_regexes=None,  # 拦截xhr接口，支持正则，数组类型
    # 即浏览器渲染相关配置 auto_install_driver 设置True，让其自动对比驱动版本，版本不符或驱动不存在时自动下载
    auto_install_driver=True,  # 自动下载浏览器驱动 支持chrome 和 firefox
    download_path=None,  # 下载文件的路径
    use_stealth_js=False,  # 使用stealth.min.js隐藏浏览器特征
)
```

#### 2.2 基本使用

```python
import time

import feapder
from feapder.utils.webdriver import WebDriver
from selenium.webdriver.common.by import By

class TestRender(feapder.AirSpider):
    def start_requests(self):
        yield feapder.Request("http://www.baidu.com", render=True)

    def parse(self, request, response):
        browser: WebDriver = response.browser
        browser.find_element(By.ID, "kw").send_keys("feapder")
        browser.find_element(By.ID, "su").click()
        print(browser.page_source)


if __name__ == "__main__":
    TestRender().start()
```

#### 2.3 拦截xhr数据

```python
WEBDRIVER = dict(
    ...
    xhr_url_regexes=[
        "接口1正则",
        "接口2正则",
    ]
)
```

#### 2.4 数据获取

```python
browser: WebDriver = response.browser
# 提取文本
text = browser.xhr_text("接口1正则")
# 提取json
data = browser.xhr_json("接口1正则")
```

#### 2.5 获取对象

```python
browser: WebDriver = response.browser
xhr_response = browser.xhr_response("接口1正则")
print("请求接口", xhr_response.request.url)
print("请求头", xhr_response.request.headers)
print("请求体", xhr_response.request.data)
print("返回头", xhr_response.headers)
print("返回地址", xhr_response.url)
print("返回内容", xhr_response.content)
```

#### 2.6 实例代码

- 配置文件

```Python
WEBDRIVER = dict(
    pool_size=1,  # 浏览器的数量
    load_images=True,  # 是否加载图片
    user_agent=None,  # 字符串 或 无参函数，返回值为user_agent
    proxy=None,  # xxx.xxx.xxx.xxx:xxxx 或 无参函数，返回值为代理地址
    headless=False,  # 是否为无头浏览器
    driver_type="CHROME",  # CHROME、PHANTOMJS、FIREFOX
    timeout=30,  # 请求超时时间
    window_size=(1024, 800),  # 窗口大小
    executable_path='D:\software\miniconda3\chromedriver.exe',  # 浏览器路径，默认为默认路径
    render_time=0,  # 渲染时长，即打开网页等待指定时间后再获取源码
    custom_argument=[
        "--ignore-certificate-errors",
        "--disable-blink-features=AutomationControlled",
    ],  # 自定义浏览器渲染参数
    xhr_url_regexes=['open/noauth/job/search'],  # 拦截xhr接口，支持正则，数组类型
    auto_install_driver=True,  # 自动下载浏览器驱动 支持chrome 和 firefox
    download_path=None,  # 下载文件的路径
    use_stealth_js=False,  # 使用stealth.min.js隐藏浏览器特征
)
```



- 爬虫代码

```python
import feapder
from feapder.utils.webdriver import WebDriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


class SpiderProject(feapder.AirSpider):
    def start_requests(self):
        yield feapder.Request("https://young.yingjiesheng.com/pc/search?jobarea=220200&keyword=Python", render=True)

    def parse(self, request, response):
        browser: WebDriver = response.browser
        wait = WebDriverWait(browser, 10)
        wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id="list"]/div[1]/div[1]/div/div[1]/div[1]/div/div[1]')))
        data = browser.xhr_json('open/noauth/job/search')
        # print(data['resultbody']['searchData']['joblist'])
        for da in data['resultbody']['searchData']['joblist']['items']:
            item = {}
            item['jobname'] = da['jobname']
            item['coname'] = da['coname']
            item['jobarea'] = da['jobarea']
            item['issuedate'] = da['issuedate']
            item['jobtag'] = da['jobtag']
            item['providesalary'] = da['providesalary']
            print(item)



if __name__ == "__main__":
    SpiderProject().start()
```



## 三、项目实战

### 1、 创建项目

```python
feapder create -p <project_name>
feapder create -p suning_startproject
```

- items： 文件夹存放与数据库表映射的item
- spiders： 文件夹存放爬虫脚本
- main.py： 运行入口
- setting.py： 爬虫配置文件 

### 2、创建爬虫文件

- 需要先切换到spiders文件夹下

```
feapder create -s sn_project
```

### 3、配置自动化

```python
WEBDRIVER = dict(
    pool_size=1,  # 浏览器的数量
    load_images=True,  # 是否加载图片
    user_agent=None,  # 字符串 或 无参函数，返回值为user_agent
    proxy=None,  # xxx.xxx.xxx.xxx:xxxx 或 无参函数，返回值为代理地址
    headless=False,  # 是否为无头浏览器
    driver_type="CHROME",  # CHROME、PHANTOMJS、FIREFOX
    timeout=30,  # 请求超时时间
    window_size=(1024, 800),  # 窗口大小
    executable_path='D:\software\miniconda3\chromedriver.exe',  # 浏览器路径，默认为默认路径
    render_time=0,  # 渲染时长，即打开网页等待指定时间后再获取源码
    custom_argument=[
        "--ignore-certificate-errors",
        "--disable-blink-features=AutomationControlled",
    ],  # 自定义浏览器渲染参数
    xhr_url_regexes=['open/noauth/job/search'],  # 拦截xhr接口，支持正则，数组类型
    auto_install_driver=True,  # 自动下载浏览器驱动 支持chrome 和 firefox
    download_path=None,  # 下载文件的路径
    use_stealth_js=False,  # 使用stealth.min.js隐藏浏览器特征
)
```

### 4、配置数据库创建item文件

#### 4.1配置数据库

```python
# MYSQL
MYSQL_IP = "localhost"
MYSQL_PORT = 3306
MYSQL_DB = "spiders"
MYSQL_USER_NAME = "root"
MYSQL_USER_PASS = "root"
```

#### 4.2创建数据库

```python
from feapder.db.mysqldb import MysqlDB
db = MysqlDB(
    ip="localhost", port=3306, db="spiders16", user_name="root", user_pass="root"
)
sql = """
CREATE TABLE `sn` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `img` varchar(255) DEFAULT NULL,
  `price` varchar(255) DEFAULT NULL,
  `title` varchar(255) DEFAULT NULL,
  `info_url` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
)
"""
db.execute(sql)
```

#### 4.3 创建item文件

进入到items文件

```
feapder create -i sn     # jd=数据库表名
```

### 5、项目源码

```python
# -*- coding: utf-8 -*-
"""
Created on 2023-03-19 22:02:02
---------
@summary:
---------
@author: 柏汌
"""
import random
import time
from lxml import etree

from items import jd_item
import feapder
from feapder.utils.webdriver import WebDriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


class JdProject(feapder.AirSpider):
    def start_requests(self):
        yield feapder.Request("https://category.vip.com/suggest.php?keyword=%E7%94%B5%E8%84%91&ff=235|12|1|1", render=True)

    def parse(self, request, response):
        browser: WebDriver = response.browser
        self.roll_page(browser)
        res = browser.page_source
        # print(res)
        html = etree.HTML(res)
        info_list = html.xpath('//section[@id="J_searchCatList"]/div[@class="c-goods-item  J-goods-item c-goods-item--auto-width"]')
        for info in info_list:
            item = {}
            item['img'] = info.xpath('./a//div[@class="c-goods-item__img"]/img/@src')[0]
            item['info_url'] = info.xpath('./a/@href')[0]
            item['price'] = info.xpath('.//div[@class="c-goods-item__sale-price J-goods-item__sale-price"]/text()')[0]
            item['title'] = info.xpath('.//div[2]/div[2]/text()')[0]
            print(item)
            yield item
        # 翻页提取
        self.click_page(browser)
        page_url = browser.current_url  # 获取点击的跳转链接
        print(page_url)
        yield feapder.Request(page_url, render=True, callback=self.parse)

    def click_page(self, browser):
        try:
            next_button = browser.find_element(By.XPATH, '//*[@id="J_nextPage_link"]')
            if next_button:
                next_button.click()
            else:
                browser.close()
        except Exception as e:
            print(e)

    def roll_page(self, browser):
        # document.body.scrollHeight查看页面高度
        for x in range(1, 15):
            time.sleep(int(random.randint(1000, 1200) / 1000))
            js = "window.scrollTo(0, {})".format(x * 1000)  # 1000 2000 3000 4000 5000
            browser.execute_script(js)



if __name__ == "__main__":
    JdProject().start()
```

