# Scrapy采集框架

## 简介

为什么学习scrapy？

1. scrapy不能解决剩下的10%的爬虫需求
2. 能够让开发过程方便、快速
3. scrapy框架能够让我们的爬虫效率更高

**Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架，我们只需要实现少量的代码，就能够快速的抓取。**

**实现流程：**![](.\image\image-20241210105014584.png)

**其流程可以描述如下：**

1. 调度器把requests-->引擎-->下载中间件--->下载器
2. 下载器发送请求，获取响应---->下载中间件---->引擎--->爬虫中间件--->爬虫
3. 爬虫提取url地址，组装成request对象---->爬虫中间件--->引擎--->调度器
4. 爬虫提取数据--->引擎--->管道
5. 管道进行数据的处理和保存

**注意：**

- 图中绿色线条的表示数据的传递
- 注意图中中间件的位置，决定了其作用
- 注意其中引擎的位置，所有的模块之前相互独立，只和引擎进行交互

**模块介绍：**

![image-20241210115423152](.\image\image-20241210115423152.png)



## 入门使用

### 1 scrapy项目实现流程

1. 创建一个scrapy项目:scrapy startproject mySpider
2. 生成一个爬虫:scrapy genspider douban movie.douban.com
3. 提取数据:完善spider，使用xpath等方法
4. 保存数据:pipeline中保存数据

### 2 创建scrapy项目

> 下面以抓取豆瓣top250来学习scrapy的入门使用：https://movie.douban.com/top250

安装scrapy命令：sudo apt-get install scrapyscrapy==2.5.0 或者：pip install scrapy==2.5.0

#### 1.创建项目面板

```python
scrapy 2.5.0 - no active project

usage:
  scrapy <command>[options] [args]

Available commands :
  bench      Run quick benchmark test #测试电脑性能
  fetch      Fetch a URL using the scrapy down1oader#将源代码下载下来并显示出来
  genspider      Generate new spider using pre-defined temp1ates#创建一个新的spider文件
  runspider      Run a self-contained spider (without creating a project)# 这个和通过craw1启动爬虫不同，scrapy runspider爬虫文件名称
  settings      Get settings values#获取当前的配置信息
  she11      Interactive scraping console#进入scrapy 的交互模式
  startproject      create new project#创建爬虫项目
  version      Print scrapy version#显示scrapy框架的版本
  view      open URL in browser，as seen by scrapy#将网页document内容下载下来，并且在浏览器显示出来
```

创建scrapy项目的命令：scrapy startproject +<项目名字>

示例：scrapy startproject myspider

生成的目录和文件结果如下：

![image-20241210144626529](.\image\image-20241210144626529.png)

### 3 创建爬虫

命令：**在项目路径下执行**:scrapy genspider +<爬虫名字> + <允许爬取的域名>

示例：

```
cd myspider
scrapy genspider douban movie.douban.com
```

生成的目录和文件结果如下：

![企业微信截图_17338138383323](D:\my_spider_mk\image\image-20241210153323433.png)

### 4 完善spider

完善spider即通过方法进行数据的提取等操作

在\myspider\myspider\douban.py中修改内容如下:

```python
import scrapy
from scrapy.http import HtmlResponse
# 自定义spider类，继承scrapy.spider
class DoubanSpider(scrapy.Spider):
    # 爬虫名字
    name = 'douban'
    # 允许爬取的范围，防止爬虫爬到别的网站
    allowed_domains = ['douban.com']
    # 开始爬取的url地址
    start_urls = ['https://movie.douban.com/top250']

    # 数据提取的方法，接受下载中间件传过来的response
    def parse(self, response:HtmlResponse, **kwargs):
        # scrapy的response对象可以直接进行xpath
        ol_list = response.xpath('//ol[@class="grid_view"]/li')
        print(ol_list)
        for ol in ol_list:
            # 创建一个数据字典
            item = {}
            # 利用scrapy封装好的xpath选择器定位元素，并通过extract()或extract_first()来获取结果
            item['title'] = ol.xpath('.//div[@class="hd"]/a/span[1]/text()').extract_first()
            item['rating'] = ol.xpath('.//div[@class="bd"]/div/span[2]/text()').extract_first()
            item['quote'] = ol.xpath('.//div[@class="bd"]//p[@class="quote"]/span/text()').extract_first()
            print(item)
```

**注意：**

1. response.xpath方法的返回结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有一些额外的方法
2. extract() 返回一个包含有字符串的列表
3. extract_first() 返回列表中的第一个字符串，列表为空没有返回None
4. spider中的parse方法必须有
5. 需要抓取的url地址必须属于allowed_domains域名下；start_urls中的url地址没有这个限制，只是一个开始的页
6. 启动爬虫的时候注意启动的位置，是在项目路径下启动

#### 4.1 response对象属性

```python
print(response.url)	#HTTP相应的 URL地址，str类型的
print(response.status)	#HTTP响应状态码，int类型的（在pycharm的控制台中你可以看到，例如200,404）
print(response.body) 	#body HTTP响应正文，bytes类型，（即网页的 HTML 源代码）
print(response.text)	#text 文本形式的HTTP响应正文，str类型，由response.body使用response.encoding解码得到
print(response.encoding)  # 查看当前编码,HTTP响应正文的编码
response.encoding = 'gbk'  # 修改编码
print(response.text)  # 使用新的编码解码文本

print(response.request)	#产生该HTTP响应的Requset对象
print(response.request.url)  # 产生该HTTP响应的Requset对象的请求的 URL

print(response.headers)	# 返回响应的 HTTP 头信息，以字典形式存储。
print(response.headers.get('Content-Type'))  # 获取 Content-Type 头

print(response.flags)	#返回与响应关联的标志列表，通常用于标记响应的特殊状态（如重定向）。类型list

response.css()	#返回值scrapy.selector.SelectorList，使用 CSS 选择器解析 HTML 内容，并返回一个 SelectorList 对象，包含所有匹配的元素。
titles = response.css('h2 a::text').getall()  # 获取所有 h2 标签下的 a 标签文本

response.xpath()	# 返回值scrapy.selector.SelectorList，使用 XPath 选择器解析 HTML 内容，并返回一个 SelectorList 对象，包含所有匹配的元素。
titles = response.xpath('//h2/a/text()').getall()  # 获取所有 h2 标签下的 a 标签文本
print(titles)

# response.follow()根据相对或绝对 URL 构建一个新的 Request 对象，并自动处理重定向和基础 URL。返回值scrapy.http.Request
# 参数：
# url：要跟随的链接，可以是相对 URL 或绝对 URL。
# callback：回调函数，用于处理新请求的响应。
# meta：传递给回调函数的元数据。
next_page = response.css('a.next::attr(href)').get()
if next_page:
    yield response.follow(next_page, self.parse)  # 跟随下一页链接

# response.follow_all()根据多个相对或绝对 URL 构建多个 Request 对象，并自动处理重定向和基础 URL。返回值list[scrapy.http.Request]
# 参数：
# urls：要跟随的链接列表，可以是相对 URL 或绝对 URL。
# callback：回调函数，用于处理新请求的响应。
# meta：传递给回调函数的元数据。
links = response.css('a.page-num::attr(href)').getall()
for link in response.follow_all(links, self.parse_page):
    yield link  # 跟随多个页面链接

# 将相对url转为绝对url。返回值str（绝对URL）
relative_url = "page/2"
absolute_url = response.urljoin(relative_url)
print(absolute_url)



# ===选择器操作：

# 从 SelectorList 中获取第一个匹配的元素的文本或属性值 str。如果没有任何匹配项，则返回 None。
title = response.css('h1::text').get()  # 获取 h1 标签的文本
print(title)

# 从 SelectorList 中获取所有匹配的元素的文本或属性值，返回一个列表。list[str]
titles = response.css('h2 a::text').getall()  # 获取所有 h2 标签下的 a 标签文本
print(titles)

# 获取匹配元素的属性字典。适用于单个元素的选择器。返回值dict
link = response.css('a.next').attrib['href']  # 获取 a 标签的 href 属性
print(link)

# 使用正则表达式从匹配的元素中提取文本。返回所有匹配的结果列表。返回值list[str]
prices = response.css('.price::text').re(r'\d+')  # 提取价格中的数字
print(prices)

# 使用正则表达式从匹配的元素中提取第一个匹配的文本。如果没有匹配项，则返回 None。返回str 或 None
price = response.css('.price::text').re_first(r'\d+')  # 提取第一个价格中的数字
print(price)



# ===其他常用方法：

# 获取传递给当前响应的 Request 对象的元数据。元数据可以在请求时通过 meta 参数传递给回调函数。
def parse(self, response):
    item = response.meta.get('item')  # 获取传递的元数据
    print(item)

# 创建当前 Response 对象的浅拷贝。如果你需要在不同的回调函数中使用相同的响应对象，可以使用此方法。返回值：scrapy.http.Response
copied_response = response.copy()

response.errback
# 设置错误回调函数，用于处理请求过程中发生的错误（如网络超时、DNS 解析失败等）。错误回调函数会接收一个 Twisted Failure 对象作为参数。
# 错误回调函数。
def parse_error(self, failure):
    self.logger.error(f"Error: {failure}")

def start_requests(self):
    yield scrapy.Request(
        url='https://example.com',
        callback=self.parse,
        errback=self.parse_error
    )
```



### 5 利用管道pipeline来处理(保存)数据

#### 5.1 对douban爬虫进行修改完善

在爬虫文件douban.py中parse()函数中最后添加

```
yield item
```

**注意：yield能够传递的对象只能是：BaseItem,Request,dict,None**

#### 5.2 修改pipelines.py文件

```python
class MyspiderPipeline:
    # 爬虫开启时打开一个文件，用于存储爬取到的数据。（爬虫开启时，就会运行该函数）
    def open_spider(self, spider):
        self.file = open('items.json', 'a',encoding="utf-8")
    
    # 关闭爬虫时关闭文件。（爬虫关闭时，就会运行该函数）
    def close_spider(self, spider):
        self.file.close()
    
    # @classmethod
    # def from_crawler(cls, crawler):
    #     settings = crawler.settings
    #     return cls(settings)

    # 爬虫文件中提取数据的方法每yield一次item，就会运行一次
    # 该方法为固定名称函数
    # item：抓取到的项目，通常是一个 Python 字典或自定义的 Item 对象。
    # spider：发送该项目的爬虫对象，可以通过它访问爬虫的属性和方法。
    # 返回值：
		# 返回处理后的项目，以便传递给下一个管道（如果有）。
		# 如果你不想将项目传递给下一个管道，可以直接返回 None。
		# 如果你想丢弃该项目，可以抛出 DropItem 异常。
    def process_item(self, item, spider):
        # 这里自定义的存储到文件，也可以自定义其他存储
        line = json.dumps(dict(item), ensure_ascii=False) + "\n"
        self.file.write(line)
        # 这里将数据返回，有scrapy自带的存储到文件的方法。下面有使用
        return item
```

#### 5.3 在settings.py设置开启pipeline

```python
# 数值越小，优先级越高。
ITEM_PIPELINES = {
   'myspider.pipelines.MyspiderPipeline': 300,
}

# 不尊搜robots.txt
# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# 带上请求头
# Override the default request headers:
DEFAULT_REQUEST_HEADERS = {
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en',
'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'
}
```

### 6 运行scrapy

#### 6.1命令运行

命令：在项目目录下执行scrapy crawl +<爬虫名字>

示例：scrapy crawl douban --nolog

#### 6.2 debug运行

```python
# scrapy封装的命令执行
from scrapy import cmdline

if __name__ == '__main__':
    # 解析的是一个列表对象
    # 获取的json文件会乱码 需要修改配置 FEED_EXPORT_ENCODING = 'utf-8'
    cmdline.execute('scrapy crawl douban -o douban.json -s FEED_EXPORT_ENCODING="utf-8"'.split())
```

**注意：**

- 安装2.5版本之后运行代码可能会遇到以下错误

```python
AttributeError: module 'OpenSSL.SSL' has no attribute 'SSLv3_METHOD'
ImportError: cannot import name 'HTTPClientFactory' from 'twisted.web.client' (unknown location)
```

- 降低OpenSSL和cryptography以及Twisted的版本

```python
pip install pyOpenSSL==22.0.0
pip install cryptography==38.0.4
pip install Twisted==20.3.0
# 或者
pip install cryptography==3.4.8
pip install pyOpenSSL==21.0.0
```

- 异步报错

```python
TypeError: ProactorEventLoop is not supported, got: <ProactorEventLoop running=False closed=False debug=False>
```

- 这个错误通常表示正在尝试在不支持 `ProactorEventLoop` 的操作系统上运行Scrapy。Scrapy目前不支持Windows上的`ProactorEventLoop`，因此您需要将其更改为`SelectorEventLoop`。
- 可以通过在Scrapy项目的settings.py文件中添加以下代码来更改默认的事件循环

```python
import asyncio
if hasattr(asyncio, "WindowsSelectorEventLoopPolicy"):
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
```

- 2.5的版本适配的是3.7的py版本,需要找到你解释器对应的版本信息
- 可以运行尝试

### 7 输出的数据日志信息

#### 7.1 日志等级

- CRITICAL：严重错误
- ERROR：一般错误
- WARNING：警告
- INFO: 一般信息
- DEBUG：调试信息

**注意：** 默认的日志等级是DEBUG

#### 7.2日志输出信息

```txt
Versions:使用的工具版本信息
Overridden settings： 重写的配置
Telnet Password：Telnet 平台密码（Scrapy附带一个内置的telnet控制台，用于检查和控制Scrapy运行过程）
Enabled extensions ：开启的拓展功能
Enabled downloader middlewares：开启的下载器中间件
Enabled spider middlewares：开启的爬虫中间件
Enabled item pipelines：开启的管道
Dumping Scrapy stats：所以的信息汇总
```

#### 7.3 日志等级设置

- 修改settings.py文件

```python
LOG_LEVEL = 'WARNING' # 设置日志显示的等级为WARNING，DEBUG输出所有
LOG_FILE = './log.txt' # 将日志信息全部记录到log.txt文件中
```

## 三、scrapy发送翻页请求

### 1 翻页请求的思路

对于要提取如下图中所有页面上的数据该怎么办？

回顾requests模块是如何实现翻页请求的：

1. 找到下一页的URL地址
2. 调用requests.get(url)

scrapy实现翻页的思路：

1. 找到下一页的url地址
2. 构造url地址的请求，传递给引擎

### 2 scrapy实现翻页请求

#### 2.1 实现方法

1. 确定url地址
2. 构造请求，scrapy.Request(url,callback)
   - callback：指定解析函数名称，表示该请求返回的响应使用哪一个函数进行解析
3. 把请求交给引擎：yield scrapy.Request(url,callback)

#### 2.2 豆瓣爬虫

> 通过爬取豆瓣top250页面的电影信息,学习如何实现翻页请求
>
> 地址：https://movie.douban.com/top250

##### 思路分析：

1. 获取首页的数据
2. 寻找下一页的地址，进行翻页，获取数据

##### 注意：

1. 可以在settings中设置ROBOTS协议

   ```
   # False表示忽略网站的robots.txt协议，默认为True
   ROBOTSTXT_OBEY = False
   ```

2. 可以在settings中设置User-Agent：

   ```
   # scrapy发送的每一个请求的默认UA都是设置的这个User-Agent
   USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'
   ```

#### 2.3 代码实现

在爬虫文件的parse方法中：

```python
    def parse(self, response, **kwargs):
        # scrapy的response对象可以直接进行xpath
        ol_list=response.xpath("//ol[@class='grid_view']/li")

        for ol in ol_list:
            item={}
            # 取豆瓣信息ban中电影标题、评分、短评
            item['title']=ol.xpath(".//div[@class='hd']/a/span[1]/text()").extract_first()
            item['rating']=ol.xpath(".//div[@class='bd']/div/span[2]/text()").extract_first()
            item['quote']=ol.xpath(".//div[@class='bd']//p[@class='quote']/span/text()").extract_first()
            item["xinxi"]="信息"
            yield item

        # 翻页
        next_page = response.xpath("//span[@class='next']/a/@href").extract_first()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)
        
```

#### 2.4 scrapy.Request的更多参数

```python
scrapy.Request(
    url
    [
     ,callback=self.函数名		# url的响应交给【函数名】去处理
     ,method="GET"		# 指定请求
     ,headers={}		# 请求头
     ,body={}		# 请求体
     ,cookies={}		# 请求cookies
     ,meta={}		# 用于往处理函数中传入参数，
        		    # 取用extra_param = response.meta.get('extra_param')
     ,dont_filter=False		#默认为False，会过滤请求的url地址，即请求过的url地址不会继续被请求，对需要重复请求的url地址可以把它设置为Ture，比如贴吧的翻页请求，页面的数据总是在变化;start_urls中的地址会被反复请求，否则程序不会启动
    ]
)
```



#### 2.5 重写start_requests方法

上述方法可以帮助我们实现翻页的问题,但是这种翻页并不是框架的最优解,我们可以重写Spider的start_requests方法,自己生成对应的请求网址给到引擎进行调度

```python
    def start_requests(self):
        for i in range(0, 10):
            url = 'https://movie.douban.com/top250?start={}&filter='.format(i * 25)
            yield scrapy.Request(url)
```

### 3 meta参数的使用

##### meta的形式:字典

##### meta的作用：meta可以实现数据在不同的解析函数中的传递

在爬虫文件的parse方法中，提取详情页增加之前callback指定的parse_detail函数：

```python
def parse(self,response):
    ...
    yield scrapy.Request(detail_url, callback=self.parse_detail,meta={"item":item})
...

def parse_detail(self,response):
    #获取之前传入的item
    item = resposne.meta["item"]
```

##### 特别注意

1. meta参数是一个字典
2. meta字典中有一个固定的键`proxy`，表示代理ip，关于代理ip的使用我们将在scrapy的下载中间件的学习中进行介绍
3. meta的download_timeout设置请求超时

### 4 item的使用

#### 4.1 Item能够做什么

1. 定义item即提前规划好哪些字段需要抓取，scrapy.Field()仅仅是提前占坑，通过item.py能够让别人清楚自己的爬虫是在抓取什么，同时定义好哪些字段是需要抓取的，没有定义的字段不能使用，防止手误
2. 在python大多数框架中，大多数框架都会自定义自己的数据类型(在python自带的数据结构基础上进行封装)，目的是增加功能，增加自定义异常

#### 4.2 定义Item

在items.py文件中定义要提取的字段：

```
class MyspiderItem(scrapy.Item):
    title = scrapy.Field()  # 标题
    rating = scrapy.Field()  # 评估
    quote = scrapy.Field()  # 概述
```

#### 4.3 使用Item

Item使用之前需要先导入并且实例化，之后的使用方法和使用字典相同

修改爬虫文件douban.py：

```python
from myspider.items import MyspiderItem  # 导入Item，注意路径
...
    def parse(self, response):
        # print(response.meta['data'])
        ol_list = response.xpath('//ol[@class="grid_view"]/li')

        for ol in ol_list:
            item = MyspiderItem()
            item['title'] = ol.xpath('.//div[@class="hd"]/a/span[1]/text()').extract_first()
            item['rating'] = ol.xpath('.//div[@class="bd"]/div/span[2]/text()').extract_first()
            item['quote'] = ol.xpath('.//div[@class="bd"]//p[@class="quote"]/span/text()').extract_first()
            # item['yeshu'] = response.meta['data']
            # print(item)
            yield item
```

##### 注意：

1. from myspider.items import MyspiderItem这一行代码中 注意item的正确导入路径，忽略pycharm标记的错误
2. python中的导入路径要诀：从哪里开始运行，就从哪里开始导入
3. 不存在的字段写入会抛出异常！！！！

## 四、scrapy的管道使用

### 1. 了解scrapyShell

 scrapy shell是scrapy提供的一个终端工具，能够通过它查看scrapy中对象的属性和方法，以及测试xpath

使用方法：

```python
scrapy shell https://movie.douban.com/top250
```

在终端输入上述命令后，能够进入python的交互式终端，此时可以使用：

- response.xpath()：直接测试xpath规则是否正确
- response.url：当前响应的url地址
- response.request.url：当前响应对应的请求的url地址
- response.headers：响应头
- response.body：响应体，也就是html代码，默认是byte类型
- response.requests.headers：当前响应的请求头

### 3 settings.py中的设置信息

##### 3.1 为什么项目中需要配置文件

 在配置文件中存放一些公共变量，在后续的项目中方便修改，如：本地测试数据库和部署服务器的数据库不一致

##### 3.2 配置文件中的变量使用方法

1. 变量名一般全部大写
2. 导入即可使用

##### 3.3 settings.py中的重点字段和含义

- USER_AGENT 设置ua
- ROBOTSTXT_OBEY 是否遵守robots协议，默认是遵守
- CONCURRENT_REQUESTS 设置并发请求的数量，默认是16个
- DOWNLOAD_DELAY 下载延迟，默认无延迟
- COOKIES_ENABLED 是否开启cookie，即每次请求带上前一次的cookie，默认是开启的
- DEFAULT_REQUEST_HEADERS 设置默认请求头
- SPIDER_MIDDLEWARES 爬虫中间件，设置过程和管道相同
- DOWNLOADER_MIDDLEWARES 下载中间件
- LOG_LEVEL 控制终端输出信息的log级别，终端默认显示的是debug级别的log信息
  - LOG_LEVEL = "WARNING"
- LOG_FILE 设置log日志文件的保存路径，如果设置该参数，终端将不再显示信息
  - LOG_FILE = "./test.log"
- 其他设置参考：https://www.jianshu.com/p/df9c0d1e9087

### 4 pipeline管道的深入使用

##### 4.1 pipeline中常用的方法：

1. process_item(self,item,spider):实现对item数据的处理
2. open_spider(self, spider): 在爬虫开启的时候仅执行一次
3. close_spider(self, spider): 在爬虫关闭的时候仅执行一次

#### 4.2 管道文件的修改

在pipelines.py代码中完善

```python
import pymysql
import pymongo


class MyspiderMySQLPipeline:
    def open_spider(self, spider):
        # 判断是哪个爬虫  名字不同可能执行的爬虫项目也不同
        if spider.name == 'douban':
            self.db = pymysql.connect(host="localhost", user="root", password="root", db="spiders9")
            self.cursor = self.db.cursor()
            # 创建变语法
            sql = '''
                                CREATE TABLE IF NOT EXISTS douban(
                                    id int primary key auto_increment not null,
                                    quote VARCHAR(255) NOT NULL, 
                                    rating VARCHAR(255) NOT NULL, 
                                    title VARCHAR(255) NOT NULL)
                                '''
            try:
                self.cursor.execute(sql)
                print("CREATE TABLE SUCCESS.")
            except Exception as ex:
                print(f"CREATE TABLE FAILED,CASE:{ex}")

    def process_item(self, item, spider):
        # SQL 插入语句
        sql = 'INSERT INTO douban(id, quote, rating, title) values(%s, %s, %s, %s)'
        # 执行 SQL 语句
        try:
            self.cursor.execute(sql, (0, item['quote'], item['rating'], item['title']))
            # 提交到数据库执行
            self.db.commit()
            print('mysql数据插入成功...')
        except Exception as e:
            print(f'数据插入失败: {e}')
            # 如果发生错误就回滚
            self.db.rollback()
        # 不return的情况下，另一个权重较低的pipeline将不会获得item,否则后一个pipeline取到的数据为None值
        return item 

    def close_spider(self, spider):  # 在爬虫关闭的时候仅执行一次
        # 关闭文件
        if spider.name == 'douban':
            self.db.close()


class MyspiderMongoDBPipeline:
    def open_spider(self, spider):  # 在爬虫开启的时候仅执行一次
        if spider.name == 'douban':
            con = pymongo.MongoClient(host='127.0.0.1', port=27017)  # 实例化mongoclient
            self.collection = con.spiders9.douban  # 创建数据库名为spiders9,集合名为douban的集合操作对象

    def process_item(self, item, spider):
        if spider.name == 'douban':
            print('mongo保存成功')
            self.collection.insert_one(dict(item))  # 此时item对象需要先转换为字典,再插入
        # 不return的情况下，另一个权重较低的pipeline将不会获得item
        return item
```

#### 4.3 开启管道

在settings.py设置开启pipeline

```python
ITEM_PIPELINES = {
   'myspider.pipelines.MyspiderMySQLPipeline': 300,
   'myspider.pipelines.MyspiderMongoDBPipeline': 400,
}
```

**思考：pipeline在settings中能够开启多个，为什么需要开启多个？**

1. 不同的pipeline可以处理不同爬虫的数据，通过spider.name属性来区分
2. 不同的pipeline能够对一个或多个爬虫进行不同的数据处理的操作，比如一个进行数据清洗，一个进行数据的保存
3. 同一个管道类也可以处理不同爬虫的数据，通过spider.name属性来区分

#### 4.4 pipeline使用注意点

1. 使用之前需要在settings中开启
2. pipeline在setting中键表示位置(即pipeline在项目中的位置可以自定义)，值表示距离引擎的远近，越近数据会越先经过
3. 有多个pipeline的时候，process_item的方法必须return item,否则后一个pipeline取到的数据为None值
4. pipeline中process_item的方法必须有，否则item没有办法接受和处理
5. process_item方法接受item和spider，其中spider表示当前传递item过来的spider
6. open_spider(spider) :能够在爬虫开启的时候执行一次
7. close_spider(spider) :能够在爬虫关闭的时候执行一次
8. 上述俩个方法经常用于爬虫和数据库的交互，在爬虫开启的时候建立和数据库的连接，在爬虫关闭的时候断开和数据库的连接

## 五、中间件

middlewares.py文件

```python
from scrapy import signals

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter


class MyspiderSpiderMiddleware:
    """
        蜘蛛中间件（Spider Middleware）主要用于处理从下载器传递给蜘蛛的响应，以及从蜘蛛传递给引擎的结果（如请求或项目）。它可以在响应进入蜘蛛之前对其进行处理，或者在蜘蛛返回结果之后对其进行处理。
    """


    @classmethod
    def from_crawler(cls, crawler):
        """
            这是一个类方法，Scrapy 会调用这个方法来创建中间件实例。它接收一个 crawler 对象作为参数，该对象包含了爬虫的所有配置和组件。
            在这个方法中，我们创建了一个中间件实例 s，并将其与 spider_opened 信号连接起来。当爬虫启动时，spider_opened 方法会被调用。
        """

        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_spider_input(self, response, spider):
        """
            当响应从下载器传递给蜘蛛时，Scrapy 会调用这个方法。你可以在这里对响应进行预处理，例如检查响应的状态码、解析响应内容等。
            如果你在这个方法中返回 None，Scrapy 会继续将响应传递给蜘蛛的回调函数。如果你抛出异常，Scrapy 会调用 process_spider_exception 方法来处理异常。
        """

        return None

    def process_spider_output(self, response, result, spider):
        """
            当蜘蛛处理完响应并返回结果（如新的请求或项目）时，Scrapy 会调用这个方法。你可以在这里对蜘蛛返回的结果进行处理，例如过滤请求、修改项目等。
            你必须返回一个可迭代的对象，其中包含 Request 或 Item 对象。
        """

        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        """
            当蜘蛛或 process_spider_input 方法抛出异常时，Scrapy 会调用这个方法。你可以在这里处理异常，例如记录日志、重试请求等。
            你可以返回 None 继续处理异常，或者返回一个新的 Response 或 Request 对象来终止异常处理链。
        """

        pass

    def process_start_requests(self, start_requests, spider):
        """
            当爬虫启动时，Scrapy 会调用这个方法来处理爬虫的起始请求。你可以在这里修改起始请求，例如添加额外的请求头、更改 URL 等。
            你必须返回一个只包含 Request 对象的可迭代对象。
        """

        for r in start_requests:
            yield r

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)


class MyspiderDownloaderMiddleware:
	"""
        下载器中间件（Downloader Middleware）主要用于处理从引擎发送到下载器的请求，以及从下载器返回的响应。它可以在请求发送之前对其进行修改，或者在响应返回之后对其进行处理。
	"""

    @classmethod
    def from_crawler(cls, crawler):
        """
        	如上效果类似
        """

        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        """
            当请求从引擎发送到下载器时，Scrapy 会调用这个方法。你可以在这里对请求进行预处理，例如添加代理、修改请求头、设置超时等。
            你可以返回 None 继续处理请求，或者返回一个新的 Response 对象（模拟响应），或者返回一个新的 Request 对象（重定向请求），或者抛出 IgnoreRequest 异常来忽略请求。
        """
        # Called for each request that goes through the downloader
        # middleware.

        # Must either:
        # - return None: continue processing this request
        # - or return a Response object
        # - or return a Request object
        # - or raise IgnoreRequest: process_exception() methods of
        #   installed downloader middleware will be called
        return None

    def process_response(self, request, response, spider):
        """
			当响应从下载器返回时，Scrapy 会调用这个方法。你可以在这里对响应进行后处理，例如修改响应内容、检查响应状态码等。
			你必须返回一个 Response 对象，或者返回一个新的 Request 对象（重定向请求），或者抛出 IgnoreRequest 异常来忽略响应。
        """
        # Called with the response returned from the downloader.

        # Must either;
        # - return a Response object
        # - return a Request object
        # - or raise IgnoreRequest
        return response

    def process_exception(self, request, exception, spider):
        """
            当下载器或 process_request 方法抛出异常时，Scrapy 会调用这个方法。你可以在这里处理异常，例如记录日志、重试请求等。
            你可以返回 None 继续处理异常，或者返回一个新的 Response 对象（模拟响应），或者返回一个新的 Request 对象（重试请求），或者抛出 IgnoreRequest 异常来忽略异常。
        """
        # Called when a download handler or a process_request()
        # (from other downloader middleware) raises an exception.

        # Must either:
        # - return None: continue processing this exception
        # - return a Response object: stops process_exception() chain
        # - return a Request object: stops process_exception() chain
        pass

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)

```

settings.py配置文件中需要启用并配置优先级

```python
# settings.py

# 启用蜘蛛中间件
SPIDER_MIDDLEWARES = {
    'mySpider.middlewares.MyspiderSpiderMiddleware': 543,
}

# 启用下载器中间件
DOWNLOADER_MIDDLEWARES = {
    'mySpider.middlewares.MyspiderDownloaderMiddleware': 543,
}
```

### 内置中间件

Scrapy 提供了一些内置的中间件，它们可以帮助你处理常见的任务，例如身份验证、重试、重定向、压缩等。你可以在 `settings.py` 中启用或禁用这些内置中间件。

- **`HttpAuthMiddleware`**：用于处理 HTTP 基本身份验证。
- **`DownloadTimeoutMiddleware`**：用于设置请求的超时时间。
- **`DefaultHeadersMiddleware`**：用于设置默认的请求头。
- **`UserAgentMiddleware`**：用于设置用户代理（User-Agent）。
- **`RetryMiddleware`**：用于自动重试失败的请求。
- **`RedirectMiddleware`**：用于处理 HTTP 重定向。
- **`HttpCompressionMiddleware`**：用于处理 Gzip 压缩的响应。
- **`CookiesMiddleware`**：用于管理 cookies。
- **`HttpProxyMiddleware`**：用于通过代理服务器发送请求。

### 中间件的优先级

中间件的优先级决定了它们的执行顺序。优先级值越小，中间件越早执行。你可以通过调整优先级值来控制中间件的执行顺序。通常，优先级范围为 0 到 1000，常用的中间件优先级如下：

- **0-400**：高优先级，通常用于处理请求和响应的核心逻辑。
- **500-600**：中等优先级，通常用于处理请求和响应的扩展功能。
- **700-1000**：低优先级，通常用于处理请求和响应的次要逻辑。

### 常见使用场景

- **身份验证**：你可以使用下载器中间件来处理身份验证，例如通过 `HttpAuthMiddleware` 或自定义中间件来添加认证信息。
- **代理切换**：你可以使用下载器中间件来动态切换代理服务器，避免 IP 被封禁。
- **请求重试**：你可以使用 `RetryMiddleware` 来自动重试失败的请求，或者编写自定义中间件来实现更复杂的重试逻辑。
- **响应缓存**：你可以编写下载器中间件来缓存响应，减少重复抓取的次数。
- **日志记录**：你可以使用蜘蛛中间件或下载器中间件来记录请求和响应的日志，帮助调试和监控爬虫的行为。
- **数据清洗**：你可以使用蜘蛛中间件来清洗蜘蛛返回的数据，例如去除重复项、格式化输出等。



### 加入一个中间件

#### 1. **使用内置的 `HttpAuthMiddleware`**

Scrapy 提供了内置的 `HttpAuthMiddleware`，它可以自动处理 HTTP 基本身份验证。你只需要在请求中传递用户名和密码即可。

##### 步骤：

1. **启用 `HttpAuthMiddleware`**： `HttpAuthMiddleware` 默认是启用的，因此你不需要在 `settings.py` 中手动添加它。如果你之前禁用了它，可以通过以下方式重新启用：

   ```python
   # settings.py
   
   DOWNLOADER_MIDDLEWARES = {
       'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,
   }
   ```

2. **在请求中传递认证信息**： 你可以在每个请求的 `meta` 参数中传递 `http_auth`，或者在 `settings.py` 中全局设置默认的用户名和密码。

   - **方法一：在每个请求中传递认证信息**：

     ```python
     import scrapy
     
     class AuthenticatedSpider(scrapy.Spider):
         name = 'authenticated_spider'
         start_urls = ['https://example.com/protected-page']
     
         def start_requests(self):
             for url in self.start_urls:
                 yield scrapy.Request(
                     url=url,
                     meta={'http_auth': {'username': 'your_username', 'password': 'your_password'}}
                 )
     ```

   - **方法二：在 `settings.py` 中全局设置默认的用户名和密码**：

     ```python
     # settings.py
     
     HTTPAUTH_USERNAME = 'your_username'
     HTTPAUTH_PASSWORD = 'your_password'
     ```

     这样，所有请求都会自动使用这些认证信息进行身份验证。

#### 2. **编写自定义的身份验证中间件**

如果你需要更复杂的认证逻辑，例如 API 密钥、OAuth 认证、或基于会话的身份验证，你可以编写自定义的下载器中间件来处理这些情况。

##### 示例 1：API 密钥认证

假设你需要在每个请求的头部添加一个 API 密钥来进行身份验证。你可以编写一个自定义的下载器中间件来实现这一点。

```python
# middlewares.py

from scrapy import signals

class ApiKeyDownloaderMiddleware:
    # This method is used by Scrapy to create your middleware.
    @classmethod
    def from_crawler(cls, crawler):
        # 获取 API 密钥的配置
        api_key = crawler.settings.get('API_KEY')
        if not api_key:
            raise ValueError("API_KEY must be set in settings.")
        
        middleware = cls(api_key)
        crawler.signals.connect(middleware.spider_opened, signal=signals.spider_opened)
        return middleware

    def __init__(self, api_key):
        self.api_key = api_key

    def process_request(self, request, spider):
        # 在每个请求的头部添加 API 密钥
        request.headers['Authorization'] = f'Bearer {self.api_key}'
        return None

    def spider_opened(self, spider):
        spider.logger.info(f'Spider opened: {spider.name} with API Key: {self.api_key}')
```

##### 启用自定义中间件：

```python
# settings.py

DOWNLOADER_MIDDLEWARES = {
    'mySpider.middlewares.ApiKeyDownloaderMiddleware': 543,
}

# 设置 API 密钥
API_KEY = 'your_api_key_here'
```

##### 示例 2：基于会话的身份验证

如果你需要通过登录页面获取会话 cookie，并在后续请求中使用该会话进行身份验证，你可以编写一个自定义的下载器中间件来管理会话。

```python
# middlewares.py

import scrapy
from scrapy.http import Request, FormRequest
from scrapy.exceptions import IgnoreRequest

class SessionDownloaderMiddleware:
    @classmethod
    def from_crawler(cls, crawler):
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def spider_opened(self, spider):
        # 登录并获取会话 cookie
        login_url = 'https://example.com/login'
        username = 'your_username'
        password = 'your_password'

        # 发送登录请求
        return scrapy.FormRequest(
            url=login_url,
            formdata={'username': username, 'password': password},
            callback=self.after_login
        )

    def after_login(self, response):
        # 检查登录是否成功
        if "Login failed" in response.body.decode('utf-8'):
            self.logger.error("Login failed")
            raise IgnoreRequest("Login failed")

        # 登录成功后继续爬取其他页面
        self.logger.info("Login successful")
        return None

    def process_request(self, request, spider):
        # 确保每个请求都带有会话 cookie
        if not request.cookies:
            request.cookies = self.session_cookies
        return None

    def process_response(self, request, response, spider):
        # 更新会话 cookie（如果需要）
        if response.status == 200 and 'Set-Cookie' in response.headers:
            self.session_cookies = response.headers.getlist('Set-Cookie')
        return response
```

##### 启用自定义中间件：

```python
# settings.py

DOWNLOADER_MIDDLEWARES = {
    'mySpider.middlewares.SessionDownloaderMiddleware': 543,
}
```

#### 3. **使用 OAuth 2.0 身份验证**

如果你需要使用 OAuth 2.0 进行身份验证，通常你需要先获取访问令牌（Access Token），然后在每个请求的头部添加该令牌。你可以编写一个自定义的下载器中间件来处理 OAuth 2.0 的认证流程。

##### 示例 3：OAuth 2.0 认证

```python
# middlewares.py

import requests
from scrapy import signals

class OAuthDownloaderMiddleware:
    @classmethod
    def from_crawler(cls, crawler):
        oauth_client_id = crawler.settings.get('OAUTH_CLIENT_ID')
        oauth_client_secret = crawler.settings.get('OAUTH_CLIENT_SECRET')
        oauth_token_url = crawler.settings.get('OAUTH_TOKEN_URL')

        if not all([oauth_client_id, oauth_client_secret, oauth_token_url]):
            raise ValueError("OAUTH_CLIENT_ID, OAUTH_CLIENT_SECRET, and OAUTH_TOKEN_URL must be set in settings.")

        middleware = cls(oauth_client_id, oauth_client_secret, oauth_token_url)
        crawler.signals.connect(middleware.spider_opened, signal=signals.spider_opened)
        return middleware

    def __init__(self, client_id, client_secret, token_url):
        self.client_id = client_id
        self.client_secret = client_secret
        self.token_url = token_url
        self.access_token = None

    def get_access_token(self):
        # 获取 OAuth 2.0 访问令牌
        data = {
            'grant_type': 'client_credentials',
            'client_id': self.client_id,
            'client_secret': self.client_secret
        }
        response = requests.post(self.token_url, data=data)
        if response.status_code == 200:
            self.access_token = response.json().get('access_token')
        else:
            raise Exception("Failed to obtain access token")

    def process_request(self, request, spider):
        # 如果没有访问令牌，先获取
        if not self.access_token:
            self.get_access_token()

        # 在每个请求的头部添加访问令牌
        request.headers['Authorization'] = f'Bearer {self.access_token}'
        return None

    def spider_opened(self, spider):
        spider.logger.info(f'Spider opened: {spider.name} with OAuth 2.0 authentication')
```

##### 启用自定义中间件：

```python
# settings.py

DOWNLOADER_MIDDLEWARES = {
    'mySpider.middlewares.OAuthDownloaderMiddleware': 543,
}

# 设置 OAuth 2.0 相关配置
OAUTH_CLIENT_ID = 'your_client_id'
OAUTH_CLIENT_SECRET = 'your_client_secret'
OAUTH_TOKEN_URL = 'https://example.com/oauth/token'
```

##### 总结

通过 Scrapy 的中间件机制，你可以轻松地实现各种身份验证方式。无论是简单的 HTTP 基本身份验证，还是复杂的 OAuth 2.0 或基于会话的身份验证，都可以通过编写自定义中间件来实现。以下是一些常见的身份验证场景及其对应的实现方式：

- **HTTP 基本身份验证**：使用内置的 `HttpAuthMiddleware` 或在请求中传递认证信息。
- **API 密钥认证**：编写自定义中间件，在每个请求的头部添加 API 密钥。
- **基于会话的身份验证**：编写自定义中间件，管理登录会话并确保每个请求都带有会话 cookie。
- **OAuth 2.0 认证**：编写自定义中间件，获取并管理访问令牌。

## 六、scrapy-redis组件学习

### 简介

Scrapy_redis ： Redis-based components for Scrapy.

Github地址：https://github.com/rmax/scrapy-redis

在这个地址中存在三个demo，后续我们对scrapy_redis的学习会通过这三个demo展开

安装方式:pip install scrapy-redis

### 组件作用

Scrapy_redis在scrapy的基础上实现了更多，更强大的功能，具体体现在：

- 请求对象的持久化
- 去重的持久化
- 和实现分布式

![image-20241212153910996](.\image\image-20241212153910996.png)

加入scrapy-redis可以实现如上结构。

#### redis配置

默认情况下，Redis服务值运行本地电脑访问，即Redis运行在哪台电脑就只允许这台电脑上的软件连接试用它

既然我们要做分布式爬取，就意味着别的电脑也需要访问Redis，因此需要对Redis进行配置

配置参考地址：https://www.cnblogs.com/masonblog/p/12726914.html

1. 别忘了修改爬虫的配置文件`settings.py`，将Redis服务器地址改为你运行Redis的服务器的ip
2. 服务器安装：https://blog.csdn.net/m0_60028455/article/details/125316625

#### redis服务端和客户端的启动

- `/etc/init.d/redis-server start` 启动服务端
- `redis-cli -h -p <端口号>` 客户端启动

#### redis中的常见命令

- 命令文档:[http://doc.redisfans.com](http://doc.redisfans.com/)
- 可视化工具:https://gitee.com/qishibo/AnotherRedisDesktopManager/releases

1. `select 1` 切换db
2. `keys *` 查看所有的键
3. `type 键` 查看键的类型
4. `flushdb` 清空db
5. `flushall` 清空数据库
6. `lrange 键 0 -1` 查看列表数据

#### redis命令的复习

redis的命令很多，这里我们简单复习后续会使用的命令

```bash
### 字符串（String）
SET mykey "Hello, Redis!"	#设置指定键的值。
GET mykey	#获取指定键的值。
INCR mycounter	#将键的值递增 1（仅适用于整数）。
DECR mycounter	#将键的值递减 1（仅适用于整数）。
INCRBY mycounter 5	#将键的值递增指定的增量。
DECRBY mycounter 3	#将键的值递减指定的减量。
APPEND mykey " World"	#将值追加到指定键的现有值后面。
STRLEN mykey	#返回指定键的值的长度。
GETRANGE mykey 0 5	#获取指定键的值的子字符串。
SETRANGE mykey 6 "Redis"	#从指定偏移量开始修改键的值。
GETSET mykey "New Value"	#设置键的值，并返回旧值。
MSET key1 "value1" key2 "value2"	#同时设置多个键的值。
MGET key1 key2	#获取多个键的值。

### 列表（List）
LPUSH mylist "one" "two" "three"	#向 mylist 从左边添加三个值，可以任意数量
RPUSH mylist "four" "five"	#将一个或多个值插入到列表的右边。
LPOP mylist	#从列表的右边弹出并返回最后一个元素。
RPOP mylist	#从列表的右边弹出并返回最后一个元素。
LRANGE mylist 0 -1	#返回mylist 中所有的值
LLEN mylist	#返回 mylist 的长度
LINDEX mylist 2	#返回列表中指定索引的元素。
LSET mylist 1 "new_value"	#设置列表中指定索引的元素。
LTRIM mylist 0 2	#修剪列表，保留指定范围内的元素。
BLPOP mylist 0	#阻塞式 LPOP，等待直到有元素可用或超时。
BRPOP mylist 0	#阻塞式 RPOP，等待直到有元素可用或超时。

### 集合（Set）
SADD myset "apple" "banana" "orange"	# 向 myset 中添加数据
SMEMBERS myset	# 获取 myset 中所有的元素
SCARD myset	# 获取 myset 中元素的数量
SISMEMBER myset "apple"	#检查集合中是否存在指定成员。
SREM myset "banana"	#从集合中移除一个或多个成员。
SPOP myset 2	#随机移除并返回一个或多个成员。
SRANDMEMBER myset 2	#随机返回一个或多个成员，但不移除。
SINTER set1 set2	#返回多个集合的交集。
SUNION set1 set2	#返回多个集合的并集。
SDIFF set1 set2	#返回多个集合的差集。
SMOVE myset1 myset2 "apple"	#将成员从一个集合移动到另一个集合。

### 有序集合（Sorted Set）
ZADD myzset 1 "one" 2 "two" 3 "three"	# 向 myzset 中添加一个值和分数
ZRANGE myzset 0 -1 WITHSCORES	#返回有序集合中指定范围的成员，按分数升序排列。
ZCARD myzset	#返回有序集合中成员的数量。
ZSCORE myzset "two"	#返回有序集合中指定成员的分数。
ZRANK myzset "two"	#返回有序集合中指定成员的排名（从 0 开始）。
ZREVRANK myzset "two"	#返回有序集合中指定成员的逆向排名（从 0 开始）。
ZRANGE myzset 0 -1 WITHSCORES	#返回有序集合中指定范围的成员，按分数升序排列。
ZREVRANGE myzset 0 -1 WITHSCORES	#返回有序集合中指定范围的成员，按分数降序排列。
ZCOUNT myzset 1 2	#返回有序集合中分数在指定范围内的成员数量。
ZREM myzset "two"	#从有序集合中移除一个或多个成员。
ZINCRBY myzset 1 "two"	#将有序集合中指定成员的分数增加指定的增量。
ZLEXCOUNT myzset - +	#返回有序集合中按字典顺序在指定范围内的成员数量。
ZRANGEBYLEX myzset - +	#返回有序集合中按字典顺序在指定范围内的成员。
ZREMRANGEBYRANK myzset 0 1	#移除有序集合中排名在指定范围内的成员。
ZREMRANGEBYSCORE myzset 1 2	#移除有序集合中分数在指定范围内的成员。
ZREMRANGEBYLEX myzset - [c	#移除有序集合中按字典顺序在指定范围内的成员。

### 哈希（Hash）
HSET user:1000 name "Alice" age 30	#设置哈希表中指定字段的值。
HGET user:1000 name	#获取哈希表中指定字段的值。
HMSET user:1000 name "Alice" age 30	#同时设置多个字段的值（已废弃，建议使用 HSET）。
HMGET user:1000 name age	#获取多个字段的值。
HGETALL user:1000	#返回哈希表中所有的字段和值。
HDEL user:1000 age	#删除一个或多个字段。
HEXISTS user:1000 name	#检查哈希表中是否存在指定字段。
HLEN user:1000	#返回哈希表中字段的数量。
HINCRBY user:1000 age 1	#返回哈希表中字段的数量。
HINCRBYFLOAT user:1000 score 0.5	#将哈希表中指定字段的值递增指定的浮点增量。
HKEYS user:1000	#返回哈希表中所有的字段。
HVALS user:1000	#返回哈希表中所有的值。

```

#### scrapy-redis的核心组件

##### 1. Scheduler（调度）：

`scrapy`改造了`python`本来的`collection.deque`(双向队列)形成了自己`scrapy queue`，而`scrapy-redis` 的解决是把这个`scrapy queue`换成`redis`数据库，从同一个`redis-server`存放要爬取的`request`，便能让多个`spider`去同一个数据库里读取。

Scrapy中跟“待爬队列”直接相关的就是调度器`Scheduler`，它负责对新的request进行入列操作（加入Scrapy queue），取出下一个要爬取的request（从Scrapy queue中取出）等操作。它把待爬队列按照优先级建立了一个字典结构，比如：

```
{
    优先级0 : 队列0
    优先级1 : 队列1
    优先级2 : 队列2
}
```

然后根据request中的优先级，来决定该入哪个队列，出列时则按优先级较小的优先出列。为了管理这个比较高级的队列字典，Scheduler需要提供一系列的方法。但是原来的Scheduler已经无法使用，所以使用Scrapy-redis的scheduler组件。

##### 2. Duplication Filter（去重）：

Scrapy中用集合实现这个request去重功能，Scrapy中把已经发送的request指纹放入到一个集合中，把下一个request的指纹拿到集合中比对，如果该指纹存在于集合中，说明这个request发送过了，如果没有则继续操作。这个核心的判重功能是这样实现的：

```python
def request_seen(self, request):
    # self.request_figerprints就是一个指纹集合  
    fp = self.request_fingerprint(request)

    # 这就是判重的核心操作  
    if fp in self.fingerprints:
        return True
    self.fingerprints.add(fp)
    if self.file:
        self.file.write(fp + os.linesep)
```

在scrapy-redis中去重是由`Duplication Filter`组件来实现的，它通过redis的set 不重复的特性，巧妙的实现了Duplication Filter去重。scrapy-redis调度器从引擎接受request，将request的指纹存⼊redis的set检查是否重复，并将不重复的request push写⼊redis的 request queue。

引擎请求request(Spider发出的）时，调度器从redis的request queue队列⾥里根据优先级pop 出⼀个request 返回给引擎，引擎将此request发给spider处理。

##### 3. Item Pipline（管道）：

引擎将(Spider返回的)爬取到的Item给`Item Pipeline`，`scrapy-redis` 的`Item Pipeline`将爬取到的 `Item `存⼊`redis`的` items queue`。

修改过`Item Pipeline`可以很方便的根据`key`从`items queue`提取`item`，从而实现 `items processes`集群。

##### 4. Base Spider（爬虫）：

不再使用`scrapy`原有的`Spider`类，重写的`RedisSpider`继承了`Spider`和`RedisMixin`这两个类，`RedisMixin`是用来从`redis`读取`url`的类。

当我们生成一个Spider继承RedisSpider时，调用setup_redis函数，这个函数会去连接redis数据库，然后会设置signals(信号)：

- 一个是当spider空闲时候的signal，会调用spider_idle函数，这个函数调用`schedule_next_request`函数，保证spider是一直活着的状态，并且抛出DontCloseSpider异常。
- 一个是当抓到一个item时的signal，会调用item_scraped函数，这个函数会调用`schedule_next_request`函数，获取下一个request。
